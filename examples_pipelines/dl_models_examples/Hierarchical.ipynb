{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= 'HAN_opt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: biotmpygpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow \n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed_value = 11111\n",
    "#seed_value = None\n",
    "\n",
    "environment_name = sys.executable.split('/')[-3]\n",
    "print('Environment:', environment_name)\n",
    "os.environ[environment_name] = str(seed_value)\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "tensorflow.random.set_seed(seed_value)\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_gpus = [0,1,2,3]\n",
    "#multiple_gpus = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "if multiple_gpus:\n",
    "    devices = []\n",
    "    for gpu in multiple_gpus:\n",
    "        devices.append('/gpu:' + str(gpu))    \n",
    "    strategy = tensorflow.distribute.MirroredStrategy(devices=devices)\n",
    "\n",
    "else:\n",
    "    # Get the GPU device name.\n",
    "    device_name = tensorflow.test.gpu_device_name()\n",
    "    # The device name should look like the following:\n",
    "    if device_name == '/device:GPU:0':\n",
    "        print('Using GPU: {}'.format(device_name))\n",
    "    else:\n",
    "        raise SystemError('GPU device not found')\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = device_name\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/malves/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/malves/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/malves/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from wrappers.bioc_wrapper import bioc_to_docs, bioc_to_relevances\n",
    "from wrappers.pandas_wrapper import relevances_to_pandas, docs_to_pandasdocs\n",
    "from preprocessing.dl import DL_preprocessing\n",
    "from mlearning.dl_models import Hierarchical_Attention_GRU, Hierarchical_Attention_LSTM,Hierarchical_Attention_LSTM2, Hierarchical_Attention_LSTM3\n",
    "from mlearning.dl_models import Hierarchical_Attention_Context, HAN_opt\n",
    "from mlearning.dl_models import DeepDTA\n",
    "from preprocessing.embeddings import compute_embedding_matrix, glove_embeddings_2\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score, auc, roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from preprocessing.dl import plot_training_history\n",
    "from preprocessing.config import Config\n",
    "from preprocessing.dl import average_precision\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from preprocessing.dl import plot_roc_n_pr_curves\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras import backend as K\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = '../datasets/PMtask_Triage_TrainingSet.xml'\n",
    "test_dataset_path = '../datasets/PMtask_Triage_TestSet.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = Config(model_name=model_name, seed_value=seed_value)\n",
    "config.stop_words = set(stopwords.words('english'))           \n",
    "#config.stop_words = None\n",
    "config.lower = True               \n",
    "config.remove_punctuation = False\n",
    "config.split_by_hyphen = True\n",
    "config.lemmatization = False           \n",
    "config.stems = False                      \n",
    "\n",
    "\n",
    "docs_train = bioc_to_docs(train_dataset_path, config=config)\n",
    "relevances_train = bioc_to_relevances(train_dataset_path, 'protein-protein')\n",
    "\n",
    "\n",
    "x_train_df = docs_to_pandasdocs(docs_train)\n",
    "y_train_df = relevances_to_pandas(x_train_df, relevances_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9685346</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10364224</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10688642</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12059041</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12897151</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22521144</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25759389</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19887646</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23486661</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22992732</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4082 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Document\n",
       "9685346   <data_structures.document.Document object at 0...\n",
       "10364224  <data_structures.document.Document object at 0...\n",
       "10688642  <data_structures.document.Document object at 0...\n",
       "12059041  <data_structures.document.Document object at 0...\n",
       "12897151  <data_structures.document.Document object at 0...\n",
       "...                                                     ...\n",
       "22521144  <data_structures.document.Document object at 0...\n",
       "25759389  <data_structures.document.Document object at 0...\n",
       "19887646  <data_structures.document.Document object at 0...\n",
       "23486661  <data_structures.document.Document object at 0...\n",
       "22992732  <data_structures.document.Document object at 0...\n",
       "\n",
       "[4082 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9685346     0\n",
       "10364224    0\n",
       "10688642    0\n",
       "12059041    0\n",
       "12897151    0\n",
       "           ..\n",
       "22521144    1\n",
       "25759389    1\n",
       "19887646    1\n",
       "23486661    1\n",
       "22992732    1\n",
       "Name: Label, Length: 4082, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'molecular basis rieger syndrome analysis pitx2 homeodomain protein activities rieger syndrome autosomal - dominant developmental disorder includes glaucoma mild craniofacial dysmorphism humans'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df['Document'][0].title_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mutations pitx2 homeobox gene linked rieger syndrome characterized wild type mutant pitx2 activities using electrophoretic mobility shift assays protein binding transient transfection assays pitx2 preferentially binds bicoid homeodomain binding site transactivates reporter genes containing site combination pitx2 another homeodomain protein pit - 1 yielded synergistic 55 - fold activation prolactin promoter transfection assays addition pit - 1 increased pitx2 binding bicoid element electrophoretic mobility shift assays furthermore demonstrate specific binding pit - 1 pitx2 vitro thus wild type pitx2 dna binding activity modulated protein - protein interactions next studied two rieger mutants threonine proline mutation t68p second helix homeodomain retained dna binding activity apparent kd 2 - fold reduction bmax however mutant transactivate reporter genes containing bicoid site mutant pitx2 protein binds pit - 1 detectable synergism prolactin promoter second mutation l54q highly conserved residue helix 1 homeodomain yielded unstable protein results provide insights potential mechanisms underlying developmental defects rieger syndrome'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df['Document'][0].abstract_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "config.padding = 'post'            #'pre' -> default; 'post' -> alternative\n",
    "config.truncating = 'post'         #'pre' -> default; 'post' -> alternative      #####\n",
    "config.oov_token = 'OOV'\n",
    "\n",
    "config.epochs = 50\n",
    "config.batch_size = 32     # e aumentar o batch\n",
    "config.learning_rate = 0.001   #experimentar diminuir\n",
    "\n",
    "config.max_sent_len = 50      #sentences will have a maximum of \"max_sent_len\" words    #400/500\n",
    "config.max_nb_words = 100_000      #it will only be considered the top \"max_nb_words\" words in the dataset\n",
    "config.max_nb_sentences = 15    # set only for the hierarchical attention model!!!\n",
    "\n",
    "config.embeddings = 'biowordvec'\n",
    "\n",
    "config.validation_percentage = 10\n",
    "\n",
    "if not os.path.isdir('./embeddings'):\n",
    "    !mkdir embeddings\n",
    "\n",
    "if config.embeddings == 'glove':\n",
    "    if not os.path.isfile('./embeddings/glove.6B.zip'):\n",
    "        # !wget -P ./embeddings http://nlp.stanford.edu/data/glove.6B.zip\n",
    "        !unzip  ./embeddings/glove.6B.zip  -d ./embeddings\n",
    "    config.embedding_path = '/embeddings/glove/glove.6B.200d.txt'\n",
    "    config.embedding_dim = 200\n",
    "    config.embedding_format = 'glove'\n",
    "\n",
    "elif config.embeddings == 'biowordvec':   #200 dimensions\n",
    "    if not os.path.isfile('./embeddings/biowordvec'):\n",
    "        !wget -O ./embeddings/biowordvec https://ndownloader.figshare.com/files/12551780\n",
    "    config.embedding_path = './embeddings/biowordvec'\n",
    "    config.embedding_dim = 200\n",
    "    config.embedding_format = 'word2vec'\n",
    "\n",
    "elif config.embeddings == 'pubmed_pmc':   #200 dimensions\n",
    "    if not os.path.isfile('./embeddings/pubmed_pmc.bin'):\n",
    "        !wget -O ./embeddings/pubmed_pmc.bin http://evexdb.org/pmresources/vec-space-models/PubMed-and-PMC-w2v.bin\n",
    "    config.embedding_path = '/embeddings/pubmed_pmc.bin'\n",
    "    config.embedding_dim = 200\n",
    "    config.embedding_format = 'word2vec'\n",
    "\n",
    "elif config.embeddings == 'pubmed_ncbi':   #100 dimensions\n",
    "    if not os.path.isfile('./embeddings/pubmed_ncbi.bin.gz'):\n",
    "        !wget -O ./embeddings/pubmed_ncbi.bin.gz ftp://ftp.ncbi.nlm.nih.gov/pub/wilbur/EMBED/pubmed_s100w10_min.bin.gz\n",
    "    config.embedding_path = '/embeddings/pubmed_ncbi.bin.gz'\n",
    "    config.embedding_dim = 100\n",
    "    config.embedding_format = 'word2vec'    \n",
    "\n",
    "else: \n",
    "    raise Exception(\"Please Insert Embeddings Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.keras_callbacks = True\n",
    "\n",
    "if config.keras_callbacks:\n",
    "    config.patience = 5   #early-stopping patience\n",
    "    checkpoint_path = str(config.model_id_path) + '/checkpoint.hdf5'\n",
    "    keras_callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=config.patience),\n",
    "           ModelCheckpoint(checkpoint_path, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "    ]\n",
    "else:\n",
    "    keras_callbacks=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "molecular basis rieger syndrome analysis pitx2 homeodomain protein activities rieger syndrome autosomal - dominant developmental disorder includes glaucoma mild craniofacial dysmorphism humans mutations pitx2 homeobox gene linked rieger syndrome characterized wild type mutant pitx2 activities using electrophoretic mobility shift assays protein binding transient transfection assays pitx2 preferentially binds bicoid homeodomain binding site transactivates reporter genes containing site combination pitx2 another homeodomain protein pit - 1 yielded synergistic 55 - fold activation prolactin promoter transfection assays addition pit - 1 increased pitx2 binding bicoid element electrophoretic mobility shift assays furthermore demonstrate specific binding pit - 1 pitx2 vitro thus wild type pitx2 dna binding activity modulated protein - protein interactions next studied two rieger mutants threonine proline mutation t68p second helix homeodomain retained dna binding activity apparent kd 2 - fold reduction bmax however mutant transactivate reporter genes containing bicoid site mutant pitx2 protein binds pit - 1 detectable synergism prolactin promoter second mutation l54q highly conserved residue helix 1 homeodomain yielded unstable protein results provide insights potential mechanisms underlying developmental defects rieger syndrome\n",
      "0 :  molecular\n",
      "1 :  basis\n",
      "2 :  rieger\n",
      "3 :  syndrome\n",
      "4 :  analysis\n",
      "5 :  pitx2\n",
      "6 :  homeodomain\n",
      "7 :  protein\n",
      "8 :  activities\n",
      "9 :  rieger\n",
      "10 :  syndrome\n",
      "11 :  autosomal\n",
      "12 :  -\n",
      "13 :  dominant\n",
      "14 :  developmental\n",
      "15 :  disorder\n",
      "16 :  includes\n",
      "17 :  glaucoma\n",
      "18 :  mild\n",
      "19 :  craniofacial\n",
      "20 :  dysmorphism\n",
      "21 :  humans\n",
      "22 :  mutations\n",
      "23 :  pitx2\n",
      "24 :  homeobox\n",
      "25 :  gene\n",
      "26 :  linked\n",
      "27 :  rieger\n",
      "28 :  syndrome\n",
      "29 :  characterized\n",
      "30 :  wild\n",
      "31 :  type\n",
      "32 :  mutant\n",
      "33 :  pitx2\n",
      "34 :  activities\n",
      "35 :  using\n",
      "36 :  electrophoretic\n",
      "37 :  mobility\n",
      "38 :  shift\n",
      "39 :  assays\n",
      "40 :  protein\n",
      "41 :  binding\n",
      "42 :  transient\n",
      "43 :  transfection\n",
      "44 :  assays\n",
      "45 :  pitx2\n",
      "46 :  preferentially\n",
      "47 :  binds\n",
      "48 :  bicoid\n",
      "49 :  homeodomain\n",
      "50 :  binding\n",
      "51 :  site\n",
      "52 :  transactivates\n",
      "53 :  reporter\n",
      "54 :  genes\n",
      "55 :  containing\n",
      "56 :  site\n",
      "57 :  combination\n",
      "58 :  pitx2\n",
      "59 :  another\n",
      "60 :  homeodomain\n",
      "61 :  protein\n",
      "62 :  pit\n",
      "63 :  -\n",
      "64 :  1\n",
      "65 :  yielded\n",
      "66 :  synergistic\n",
      "67 :  55\n",
      "68 :  -\n",
      "69 :  fold\n",
      "70 :  activation\n",
      "71 :  prolactin\n",
      "72 :  promoter\n",
      "73 :  transfection\n",
      "74 :  assays\n",
      "75 :  addition\n",
      "76 :  pit\n",
      "77 :  -\n",
      "78 :  1\n",
      "79 :  increased\n",
      "80 :  pitx2\n",
      "81 :  binding\n",
      "82 :  bicoid\n",
      "83 :  element\n",
      "84 :  electrophoretic\n",
      "85 :  mobility\n",
      "86 :  shift\n",
      "87 :  assays\n",
      "88 :  furthermore\n",
      "89 :  demonstrate\n",
      "90 :  specific\n",
      "91 :  binding\n",
      "92 :  pit\n",
      "93 :  -\n",
      "94 :  1\n",
      "95 :  pitx2\n",
      "96 :  vitro\n",
      "97 :  thus\n",
      "98 :  wild\n",
      "99 :  type\n",
      "100 :  pitx2\n",
      "101 :  dna\n",
      "102 :  binding\n",
      "103 :  activity\n",
      "104 :  modulated\n",
      "105 :  protein\n",
      "106 :  -\n",
      "107 :  protein\n",
      "108 :  interactions\n",
      "109 :  next\n",
      "110 :  studied\n",
      "111 :  two\n",
      "112 :  rieger\n",
      "113 :  mutants\n",
      "114 :  threonine\n",
      "115 :  proline\n",
      "116 :  mutation\n",
      "117 :  t68p\n",
      "118 :  second\n",
      "119 :  helix\n",
      "120 :  homeodomain\n",
      "121 :  retained\n",
      "122 :  dna\n",
      "123 :  binding\n",
      "124 :  activity\n",
      "125 :  apparent\n",
      "126 :  kd\n",
      "127 :  2\n",
      "128 :  -\n",
      "129 :  fold\n",
      "130 :  reduction\n",
      "131 :  bmax\n",
      "132 :  however\n",
      "133 :  mutant\n",
      "134 :  transactivate\n",
      "135 :  reporter\n",
      "136 :  genes\n",
      "137 :  containing\n",
      "138 :  bicoid\n",
      "139 :  site\n",
      "140 :  mutant\n",
      "141 :  pitx2\n",
      "142 :  protein\n",
      "143 :  binds\n",
      "144 :  pit\n",
      "145 :  -\n",
      "146 :  1\n",
      "147 :  detectable\n",
      "148 :  synergism\n",
      "149 :  prolactin\n",
      "150 :  promoter\n",
      "151 :  second\n",
      "152 :  mutation\n",
      "153 :  l54q\n",
      "154 :  highly\n",
      "155 :  conserved\n",
      "156 :  residue\n",
      "157 :  helix\n",
      "158 :  1\n",
      "159 :  homeodomain\n",
      "160 :  yielded\n",
      "161 :  unstable\n",
      "162 :  protein\n",
      "163 :  results\n",
      "164 :  provide\n",
      "165 :  insights\n",
      "166 :  potential\n",
      "167 :  mechanisms\n",
      "168 :  underlying\n",
      "169 :  developmental\n",
      "170 :  defects\n",
      "171 :  rieger\n",
      "172 :  syndrome\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing for Training Data\n",
    "\n",
    "config.tokenizer = text.Tokenizer(num_words=config.max_nb_words, oov_token=config.oov_token)\n",
    "\n",
    "print(x_train_df['Document'][0].fulltext_string)\n",
    "for i, tok in enumerate(x_train_df['Document'][0].fulltext_tokens):\n",
    "    print(i, ': ', tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30020 unique tokens.\n",
      "Index of Unknown Words: 1\n",
      "Training set with 3674 samples\n",
      "Validation set with 408 samples\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val = DL_preprocessing(x_train_df, y_train_df, \n",
    "                                                  config, dataset='train',\n",
    "                                                  validation_percentage = config.validation_percentage, \n",
    "                                                  seed_value=config.seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   6,  786,    1,  115,  786,    1,   17,  786,    1,  327,   72,\n",
       "       3282,    8,  225,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 3674\n",
      "Number of Sentences:  15\n",
      "Max number of words in a Sentence: 50\n",
      "Percentage of Zeros: 79.14%\n",
      "[   6  786    1  115  786    1   17  786    1  327   72 3282    8  225\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "[ 616  364  114 6607 1603  746   40  189   83  370    8  225  628    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "[ 786    1   17  786    1  115   23 7584    7  155  786    1  327 1737\n",
      "    7  225    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "[ 786    1  327  212  833  386    1 1798  347  386    1   14   62    8\n",
      "  225  661   56   96  489   23  786    1  115    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "[  178   104   153   786     1   115  1786    96   786     1    17   212\n",
      "  4856  1318     1  3148     1  4818   244 11812 11684     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "[   99   786     1   115   204   313    31     8   225    61  3237   824\n",
      "   786     1   327   386     1    14    62   786     1    17 11812    62\n",
      "   423     1   523   664  1928     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('Number of Documents:', len(x_train))\n",
    "print('Number of Sentences: ', len(x_train[0]))\n",
    "print('Max number of words in a Sentence:', len(x_train[0][0]))\n",
    "\n",
    "count_zeros = total_words = 0\n",
    "for doc in x_train:\n",
    "    for sentence in doc:\n",
    "        for word in sentence:\n",
    "            total_words += 1\n",
    "            if word == 0:\n",
    "                count_zeros += 1\n",
    "print('Percentage of Zeros: {:.2%}'.format(count_zeros/total_words))\n",
    "\n",
    "\n",
    "for i in x_train[0]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Embedding Matrix...\n",
      "Embedding Matrix Created \n",
      "------------------------\n",
      "number of null word embeddings: 3025 in a total of 30020 words (10.08%)\n",
      "words not found: 0\n"
     ]
    }
   ],
   "source": [
    "config.embedding_matrix = compute_embedding_matrix(config, embeddings_format = config.embedding_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30021, 200)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the word embedding for the word 'protein':  200\n",
      "Index for the word 'protein':  2\n"
     ]
    }
   ],
   "source": [
    "protein_index = config.tokenizer.word_index['protein']\n",
    "print(\"Dimension of the word embedding for the word 'protein': \", len(config.embedding_matrix[protein_index]))\n",
    "print(\"Index for the word 'protein': \", protein_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 15, 50)]          0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 15, 256)           6407144   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 15, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 15, 512)           1050624   \n",
      "_________________________________________________________________\n",
      "attention_with_context_1 (At (None, 512)               263168    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 7,721,449\n",
      "Trainable params: 1,717,249\n",
      "Non-trainable params: 6,004,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if multiple_gpus:\n",
    "    with strategy.scope():\n",
    "        model = HAN_opt(config.embedding_matrix, config, learning_rate=config.learning_rate,\n",
    "                                               seed_value=config.seed_value) \n",
    "else:\n",
    "    model = HAN_opt(config.embedding_matrix, config, learning_rate=config.learning_rate,\n",
    "                                                seed_value=config.seed_value) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "INFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "113/115 [============================>.] - ETA: 0s - accuracy: 0.6206 - loss: 0.6483INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60003, saving model to ../pipelines/models/HAN_opt/HAN_opt_43/checkpoint.hdf5\n",
      "115/115 [==============================] - 10s 87ms/step - accuracy: 0.6198 - loss: 0.6493 - val_accuracy: 0.6544 - val_loss: 0.6000\n",
      "Epoch 2/50\n",
      "113/115 [============================>.] - ETA: 0s - accuracy: 0.7138 - loss: 0.5530\n",
      "Epoch 00002: val_loss improved from 0.60003 to 0.52343, saving model to ../pipelines/models/HAN_opt/HAN_opt_43/checkpoint.hdf5\n",
      "115/115 [==============================] - 4s 30ms/step - accuracy: 0.7145 - loss: 0.5530 - val_accuracy: 0.7525 - val_loss: 0.5234\n",
      "Epoch 3/50\n",
      " 38/115 [========>.....................] - ETA: 1s - accuracy: 0.7270 - loss: 0.5352"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=config.epochs,\n",
    "                    batch_size=config.batch_size,\n",
    "                    validation_data=(x_val,y_val),\n",
    "                    callbacks=keras_callbacks)\n",
    "\n",
    "if config.keras_callbacks:\n",
    "    model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss, config.train_acc = model.evaluate(x_train, y_train, verbose=0, batch_size = config.batch_size)\n",
    "\n",
    "print('Training Loss: %.3f' % (train_loss))\n",
    "print('Training Accuracy: %.3f' % (config.train_acc))\n",
    "\n",
    "plot_training_history(history_dict = history, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test = bioc_to_docs(test_dataset_path, config=config)\n",
    "relevances_test = bioc_to_relevances(test_dataset_path, 'protein-protein')\n",
    "\n",
    "x_test_df = docs_to_pandasdocs(docs_test)\n",
    "y_test_df = relevances_to_pandas(x_test_df, relevances_test)\n",
    "\n",
    "x_test, y_test = DL_preprocessing(x_test_df, y_test_df, config, dataset = 'test')\n",
    "\n",
    "nmr_unknown_words=0\n",
    "total_words = 0\n",
    "for doc in x_test:\n",
    "    for sentence in doc:\n",
    "        for word in sentence:\n",
    "            if word != 0:\n",
    "                total_words += 1\n",
    "            if word == 1:\n",
    "                nmr_unknown_words+=1\n",
    "\n",
    "print('Percentage of Unknown Words on the Test Set: {:.2%} ({} in {})'.format(nmr_unknown_words/total_words, nmr_unknown_words,total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = model.predict(x_test, verbose=0)\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "\n",
    "yhat_classes = np.where(yhat_probs > 0.5, 1, yhat_probs)\n",
    "yhat_classes = np.where(yhat_classes < 0.5, 0, yhat_classes).astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC and Precision-Recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config.test_roc_auc, config.test_pr_auc = plot_roc_n_pr_curves(y_test, yhat_probs,config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config.test_avg_prec  = average_precision(y_test_df, yhat_probs)\n",
    "print('Average Precision: %f' % config.test_avg_prec)\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "config.test_acc = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % config.test_acc)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "config.test_prec = precision_score(y_test, yhat_classes)\n",
    "print('Precision: %f' % config.test_prec)\n",
    "\n",
    "# recall: tp / (tp + fn)\n",
    "config.test_recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %f' % config.test_recall)\n",
    "\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "config.test_f1_score = f1_score(y_test, yhat_classes)\n",
    "print('F1 score: %f' % config.test_f1_score)\n",
    "\n",
    "# ROC AUC\n",
    "print('ROC AUC: %f' % config.test_roc_auc)\n",
    "\n",
    "# PR AUC\n",
    "print('PR AUC: %f' % config.test_pr_auc)\n",
    "\n",
    "# kappa\n",
    "config.test_kappa = cohen_kappa_score(y_test, yhat_classes)\n",
    "print('Cohens kappa: %f' % config.test_kappa)\n",
    "\n",
    "config.test_mcc = matthews_corrcoef(y_test, yhat_classes)\n",
    "print('MCC: %f' % config.test_mcc)\n",
    "\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(y_test, yhat_classes)\n",
    "print('Confusion Matrix:\\n %s \\n' % matrix)\n",
    "\n",
    "config.test_true_neg, config.test_false_pos, config.test_false_neg, config.test_true_pos = confusion_matrix(\n",
    "                                                                                                y_test, yhat_classes).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.write_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(config.model_id_path / 'model_tf', save_format = 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_yaml = model.to_yaml()\n",
    "with open(config.model_id_path / \"model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
