{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= 'bert_dense_ft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: biotmpygpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow \n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed_value = 123123\n",
    "#seed_value = None\n",
    "\n",
    "environment_name = sys.executable.split('/')[-3]\n",
    "print('Environment:', environment_name)\n",
    "os.environ[environment_name] = str(seed_value)\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "tensorflow.random.set_seed(seed_value)\n",
    "\n",
    "import torch\n",
    "if seed_value:\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_gpus = [0,1,2,3]\n",
    "#multiple_gpus = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "if multiple_gpus:\n",
    "    devices = []\n",
    "    for gpu in multiple_gpus:\n",
    "        devices.append('/gpu:' + str(gpu))    \n",
    "    strategy = tensorflow.distribute.MirroredStrategy(devices=devices)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''\n",
    "\n",
    "else:\n",
    "    # Get the GPU device name.\n",
    "    device_name = tensorflow.test.gpu_device_name()\n",
    "    # The device name should look like the following:\n",
    "    if device_name == '/device:GPU:0':\n",
    "        print('Using GPU: {}'.format(device_name))\n",
    "    else:\n",
    "        raise SystemError('GPU device not found')\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = device_name\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/malves/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/malves/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/malves/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from wrappers.bioc_wrapper import bioc_to_docs, bioc_to_relevances\n",
    "from wrappers.pandas_wrapper import relevances_to_pandas, docs_to_pandasdocs\n",
    "from preprocessing.dl import DL_preprocessing\n",
    "from mlearning.dl_models import Bert_Dense, Bert_LSTM, Bert_CLS, Bert_Sequence\n",
    "from preprocessing.dl import Bert_preprocessing\n",
    "from preprocessing.embeddings import compute_embedding_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import matthews_corrcoef, cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score, auc, roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from preprocessing.dl_config import DLConfig\n",
    "from preprocessing.dl import average_precision\n",
    "from preprocessing.dl import plot_roc_n_pr_curves, plot_training_history\n",
    "from transformers import BertTokenizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset_path = '../datasets/PMtask_Triage_TrainingSet.xml'\n",
    "test_dataset_path = '../datasets/PMtask_Triage_TestSet.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dl_config = DLConfig(model_name=model_name, seed_value=seed_value)\n",
    "\n",
    "#dl_config.stop_words = set(stopwords.words('english'))            #####\n",
    "dl_config.stop_words = None\n",
    "dl_config.lower = False                #####\n",
    "dl_config.remove_punctuation = False\n",
    "dl_config.split_by_hyphen = False   \n",
    "dl_config.lemmatization = False           #####\n",
    "dl_config.stems = False                       #####\n",
    "\n",
    "\n",
    "docs_train = bioc_to_docs(train_dataset_path, dl_config=dl_config)\n",
    "relevances_train = bioc_to_relevances(train_dataset_path, 'protein-protein')\n",
    "\n",
    "\n",
    "x_train_df = docs_to_pandasdocs(docs_train)\n",
    "y_train_df = relevances_to_pandas(x_train_df, relevances_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9685346</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10364224</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10688642</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12059041</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12897151</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22521144</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25759389</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19887646</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23486661</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22992732</th>\n",
       "      <td>&lt;data_structures.document.Document object at 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4082 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Document\n",
       "9685346   <data_structures.document.Document object at 0...\n",
       "10364224  <data_structures.document.Document object at 0...\n",
       "10688642  <data_structures.document.Document object at 0...\n",
       "12059041  <data_structures.document.Document object at 0...\n",
       "12897151  <data_structures.document.Document object at 0...\n",
       "...                                                     ...\n",
       "22521144  <data_structures.document.Document object at 0...\n",
       "25759389  <data_structures.document.Document object at 0...\n",
       "19887646  <data_structures.document.Document object at 0...\n",
       "23486661  <data_structures.document.Document object at 0...\n",
       "22992732  <data_structures.document.Document object at 0...\n",
       "\n",
       "[4082 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9685346     0\n",
       "10364224    0\n",
       "10688642    0\n",
       "12059041    0\n",
       "12897151    0\n",
       "           ..\n",
       "22521144    1\n",
       "25759389    1\n",
       "19887646    1\n",
       "23486661    1\n",
       "22992732    1\n",
       "Name: Label, Length: 4082, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The molecular basis of Rieger syndrome . Analysis of Pitx2 homeodomain protein activities .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df['Document'][0].title_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rieger syndrome is an autosomal-dominant developmental disorder that includes glaucoma and mild craniofacial dysmorphism in humans . Mutations in the Pitx2 homeobox gene have been linked to Rieger syndrome . We have characterized wild type and mutant Pitx2 activities using electrophoretic mobility shift assays , protein binding , and transient transfection assays . Pitx2 preferentially binds the bicoid homeodomain binding site and transactivates reporter genes containing this site . The combination of Pitx2 and another homeodomain protein , Pit-1 , yielded a synergistic 55-fold activation of the prolactin promoter in transfection assays . Addition of Pit-1 increased Pitx2 binding to the bicoid element in electrophoretic mobility shift assays . Furthermore , we demonstrate specific binding of Pit-1 to Pitx2 in vitro . Thus , wild type Pitx2 DNA binding activity is modulated by protein-protein interactions . We next studied two Rieger mutants . A threonine to proline mutation ( T68P ) in the second helix of the homeodomain retained DNA binding activity with the same apparent KD and only about a 2-fold reduction in the Bmax . However , this mutant did not transactivate reporter genes containing the bicoid site . The mutant Pitx2 protein binds Pit-1 , but there was no detectable synergism on the prolactin promoter . A second mutation ( L54Q ) in a highly conserved residue in helix 1 of the homeodomain yielded an unstable protein . Our results provide insights into the potential mechanisms underlying the developmental defects in Rieger syndrome .'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df['Document'][0].abstract_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "dl_config.padding = 'post'           \n",
    "dl_config.truncating = 'post'        \n",
    "\n",
    "dl_config.epochs = 3         # recommended number of epochs: 2, 3, 4 \n",
    "dl_config.batch_size = 16     # recommended batch-size: 16 or 32 \n",
    "dl_config.learning_rate = 2e-5   # recommended learning rate for Adam: 5e-5, 3e-5, 2e-5   # 3e-4, 1e-4,\n",
    "\n",
    "dl_config.max_sent_len = 512      #sentences will have a maximum of \"max_sent_len\" words\n",
    "dl_config.nmr_sentences = 1      #[1 or 2]\n",
    "\n",
    "dl_config.validation_percentage = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_config.keras_callbacks = False\n",
    "\n",
    "if dl_config.keras_callbacks:\n",
    "    dl_config.patience = 2   #early-stopping patience\n",
    "    checkpoint_path = str(dl_config.model_id_path) + '/checkpoint.hdf5'\n",
    "    keras_callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=dl_config.patience),\n",
    "            ModelCheckpoint(checkpoint_path, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "    ]\n",
    "else:\n",
    "    keras_callbacks=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_config.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_config.tokenizer.convert_tokens_to_ids(\"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with 3674 samples\n",
      "Validation set with 408 samples\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val =  Bert_preprocessing(x_train_df, y_train_df, \n",
    "                                                     dl_config, \n",
    "                                                     nmr_sentences = dl_config.nmr_sentences, \n",
    "                                                     validation_percentage = dl_config.validation_percentage, \n",
    "                                                     seed_value=dl_config.seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'crystal',\n",
       " 'structure',\n",
       " 'analysis',\n",
       " 'of',\n",
       " 'the',\n",
       " 'phd',\n",
       " 'domain',\n",
       " 'of',\n",
       " 'the',\n",
       " 'transcription',\n",
       " 'co',\n",
       " '-',\n",
       " 'act',\n",
       " '##iva',\n",
       " '##tor',\n",
       " 'p',\n",
       " '##y',\n",
       " '##go',\n",
       " '##pus',\n",
       " '.',\n",
       " 'the',\n",
       " 'w',\n",
       " '##nt',\n",
       " '/',\n",
       " 'beta',\n",
       " '-',\n",
       " 'cat',\n",
       " '##eni',\n",
       " '##n',\n",
       " 'signaling',\n",
       " 'pathway',\n",
       " 'plays',\n",
       " 'important',\n",
       " 'roles',\n",
       " 'in',\n",
       " 'animal',\n",
       " 'development',\n",
       " 'and',\n",
       " 'cancer',\n",
       " '.',\n",
       " 'p',\n",
       " '##y',\n",
       " '##go',\n",
       " '##pus',\n",
       " '(',\n",
       " 'p',\n",
       " '##y',\n",
       " '##go',\n",
       " ')',\n",
       " 'and',\n",
       " 'leg',\n",
       " '##less',\n",
       " '(',\n",
       " 'l',\n",
       " '##gs',\n",
       " ')',\n",
       " 'are',\n",
       " 'recently',\n",
       " 'discovered',\n",
       " 'core',\n",
       " 'components',\n",
       " 'of',\n",
       " 'the',\n",
       " 'w',\n",
       " '##nt',\n",
       " '/',\n",
       " 'beta',\n",
       " '-',\n",
       " 'cat',\n",
       " '##eni',\n",
       " '##n',\n",
       " 'transcription',\n",
       " 'machinery',\n",
       " 'complex',\n",
       " ',',\n",
       " 'and',\n",
       " 'are',\n",
       " 'crucial',\n",
       " '##ly',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'the',\n",
       " 'regulation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'transcription',\n",
       " 'of',\n",
       " 'the',\n",
       " 'arm',\n",
       " '/',\n",
       " 'beta',\n",
       " '-',\n",
       " 'cat',\n",
       " '##eni',\n",
       " '##n',\n",
       " 'and',\n",
       " 't',\n",
       " 'cell',\n",
       " 'factors',\n",
       " '(',\n",
       " 'tc',\n",
       " '##f',\n",
       " ')',\n",
       " '.',\n",
       " 'l',\n",
       " '##gs',\n",
       " '/',\n",
       " 'bc',\n",
       " '##l',\n",
       " '##9',\n",
       " 'functions',\n",
       " 'as',\n",
       " 'an',\n",
       " 'adapt',\n",
       " '##or',\n",
       " 'between',\n",
       " 'p',\n",
       " '##y',\n",
       " '##go',\n",
       " 'and',\n",
       " 'arm',\n",
       " '/',\n",
       " 'beta',\n",
       " '-',\n",
       " 'cat',\n",
       " '##eni',\n",
       " '##n',\n",
       " '.',\n",
       " 'here',\n",
       " ',',\n",
       " 'we',\n",
       " 'report',\n",
       " 'the',\n",
       " 'first',\n",
       " 'crystal',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'plant',\n",
       " 'home',\n",
       " '##od',\n",
       " '##oma',\n",
       " '##in',\n",
       " '(',\n",
       " 'phd',\n",
       " ')',\n",
       " 'finger',\n",
       " 'of',\n",
       " 'p',\n",
       " '##y',\n",
       " '##go',\n",
       " '##pus',\n",
       " '(',\n",
       " 'p',\n",
       " '##y',\n",
       " '##go',\n",
       " '##1',\n",
       " 'phd',\n",
       " ')',\n",
       " ',',\n",
       " 'a',\n",
       " 'p',\n",
       " '##y',\n",
       " '##go',\n",
       " 'family',\n",
       " 'member',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'essential',\n",
       " 'for',\n",
       " 'the',\n",
       " 'association',\n",
       " 'with',\n",
       " 'l',\n",
       " '##gs',\n",
       " '/',\n",
       " 'bc',\n",
       " '##l',\n",
       " '##9',\n",
       " '.',\n",
       " 'the',\n",
       " 'p',\n",
       " '##y',\n",
       " '##go',\n",
       " '##1',\n",
       " 'phd',\n",
       " 'structure',\n",
       " 'forms',\n",
       " 'a',\n",
       " 'canonical',\n",
       " 'phd',\n",
       " 'finger',\n",
       " 'motif',\n",
       " ',',\n",
       " 'stabilized',\n",
       " 'by',\n",
       " 'two',\n",
       " 'z',\n",
       " '##n',\n",
       " 'ions',\n",
       " 'coordinated',\n",
       " 'in',\n",
       " 'a',\n",
       " 'cross',\n",
       " '-',\n",
       " 'brace',\n",
       " 'scheme',\n",
       " '.',\n",
       " 'surprisingly',\n",
       " ',',\n",
       " 'the',\n",
       " 'p',\n",
       " '##y',\n",
       " '##go',\n",
       " '##1',\n",
       " 'phd',\n",
       " 'domain',\n",
       " 'forms',\n",
       " 'a',\n",
       " 'dime',\n",
       " '##r',\n",
       " 'in',\n",
       " 'both',\n",
       " 'the',\n",
       " 'crystals',\n",
       " 'and',\n",
       " 'solution',\n",
       " '.',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'structural',\n",
       " 'evidence',\n",
       " 'for',\n",
       " 'dime',\n",
       " '##rization',\n",
       " 'among',\n",
       " 'the',\n",
       " 'known',\n",
       " 'phd',\n",
       " 'domain',\n",
       " 'structures',\n",
       " '.',\n",
       " 'the',\n",
       " 'dime',\n",
       " '##r',\n",
       " 'formation',\n",
       " 'occurs',\n",
       " 'by',\n",
       " 'the',\n",
       " 'interactions',\n",
       " 'of',\n",
       " 'anti',\n",
       " '##para',\n",
       " '##lle',\n",
       " '##l',\n",
       " 'beta',\n",
       " '-',\n",
       " 'sheets',\n",
       " 'between',\n",
       " 'the',\n",
       " 'symmetry',\n",
       " '-',\n",
       " 'related',\n",
       " 'beta',\n",
       " '##3',\n",
       " 'strands',\n",
       " 'of',\n",
       " 'the',\n",
       " 'mono',\n",
       " '##mers',\n",
       " '.',\n",
       " 'the',\n",
       " 'p',\n",
       " '##y',\n",
       " '##go',\n",
       " '##1',\n",
       " 'phd',\n",
       " 'dime',\n",
       " '##r',\n",
       " 'interface',\n",
       " 'mainly',\n",
       " 'comprises',\n",
       " 'hydro',\n",
       " '##phobic',\n",
       " 'residues',\n",
       " '.',\n",
       " 'interesting',\n",
       " '##ly',\n",
       " ',',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'interface',\n",
       " 'residues',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'met',\n",
       " '##37',\n",
       " '##2',\n",
       " ',',\n",
       " 'th',\n",
       " '##r',\n",
       " '##37',\n",
       " '##3',\n",
       " ',',\n",
       " 'ala',\n",
       " '##37',\n",
       " '##6',\n",
       " 'and',\n",
       " 'le',\n",
       " '##u',\n",
       " '##38',\n",
       " '##0',\n",
       " ',',\n",
       " 'are',\n",
       " 'reportedly',\n",
       " 'important',\n",
       " 'for',\n",
       " 'the',\n",
       " 'association',\n",
       " 'with',\n",
       " 'l',\n",
       " '##gs',\n",
       " '/',\n",
       " 'bc',\n",
       " '##l',\n",
       " '##9',\n",
       " 'and',\n",
       " 'are',\n",
       " 'also',\n",
       " 'critical',\n",
       " 'for',\n",
       " 'transcription',\n",
       " '##al',\n",
       " 'activation',\n",
       " '.',\n",
       " 'the',\n",
       " 'm3',\n",
       " '##7',\n",
       " '##2',\n",
       " '##a',\n",
       " 'and',\n",
       " 'l',\n",
       " '##38',\n",
       " '##0',\n",
       " '##d',\n",
       " 'mutants',\n",
       " ',',\n",
       " 'and',\n",
       " 'several',\n",
       " 'surrounding',\n",
       " 'mutants',\n",
       " 'such',\n",
       " 'as',\n",
       " 's',\n",
       " '##38',\n",
       " '##5',\n",
       " '##a',\n",
       " 'and',\n",
       " 'a',\n",
       " '##38',\n",
       " '##6',\n",
       " '##d',\n",
       " ',',\n",
       " 'showed',\n",
       " 'decreased',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'form',\n",
       " 'dime',\n",
       " '##rs',\n",
       " 'and',\n",
       " 'to',\n",
       " 'interact',\n",
       " 'with',\n",
       " 'the',\n",
       " 'homo',\n",
       " '##logy',\n",
       " 'domain',\n",
       " '1',\n",
       " '(',\n",
       " 'hd',\n",
       " '##1',\n",
       " ')',\n",
       " 'of',\n",
       " 'l',\n",
       " '##gs',\n",
       " '/',\n",
       " 'bc',\n",
       " '##l',\n",
       " '##9',\n",
       " '.',\n",
       " 'these',\n",
       " 'results',\n",
       " 'suggest',\n",
       " 'that',\n",
       " 'the',\n",
       " 'p',\n",
       " '##y',\n",
       " '##go',\n",
       " '##1',\n",
       " 'phd',\n",
       " 'dime',\n",
       " '##rization',\n",
       " 'is',\n",
       " 'functional',\n",
       " '##ly',\n",
       " 'important',\n",
       " 'for',\n",
       " 'l',\n",
       " '##gs',\n",
       " '/',\n",
       " 'bc',\n",
       " '##l',\n",
       " '##9',\n",
       " 'recognition',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'for',\n",
       " 'the',\n",
       " 'regulation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'w',\n",
       " '##nt',\n",
       " '/',\n",
       " 'beta',\n",
       " '-',\n",
       " 'cat',\n",
       " '##eni',\n",
       " '##n',\n",
       " 'signaling',\n",
       " 'pathway',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_config.tokenizer.convert_ids_to_tokens(x_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'the',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'mouse',\n",
       " 'hp',\n",
       " '##1',\n",
       " 'suggests',\n",
       " 'a',\n",
       " 'unique',\n",
       " 'mode',\n",
       " 'of',\n",
       " 'single',\n",
       " 'peptide',\n",
       " 'recognition',\n",
       " 'by',\n",
       " 'the',\n",
       " 'shadow',\n",
       " 'ch',\n",
       " '##rom',\n",
       " '##o',\n",
       " 'domain',\n",
       " 'dime',\n",
       " '##r',\n",
       " '.',\n",
       " 'the',\n",
       " 'het',\n",
       " '##ero',\n",
       " '##ch',\n",
       " '##rom',\n",
       " '##atin',\n",
       " 'protein',\n",
       " '1',\n",
       " '(',\n",
       " 'hp',\n",
       " '##1',\n",
       " ')',\n",
       " 'family',\n",
       " 'of',\n",
       " 'proteins',\n",
       " 'is',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'gene',\n",
       " 'si',\n",
       " '##len',\n",
       " '##cing',\n",
       " 'via',\n",
       " 'the',\n",
       " 'formation',\n",
       " 'of',\n",
       " 'het',\n",
       " '##ero',\n",
       " '##ch',\n",
       " '##romatic',\n",
       " 'structures',\n",
       " '.',\n",
       " 'they',\n",
       " 'are',\n",
       " 'composed',\n",
       " 'of',\n",
       " 'two',\n",
       " 'related',\n",
       " 'domains',\n",
       " ':',\n",
       " 'an',\n",
       " 'n',\n",
       " '-',\n",
       " 'terminal',\n",
       " 'ch',\n",
       " '##rom',\n",
       " '##o',\n",
       " 'domain',\n",
       " 'and',\n",
       " 'a',\n",
       " 'c',\n",
       " '-',\n",
       " 'terminal',\n",
       " 'shadow',\n",
       " 'ch',\n",
       " '##rom',\n",
       " '##o',\n",
       " 'domain',\n",
       " '.',\n",
       " 'present',\n",
       " 'results',\n",
       " 'suggest',\n",
       " 'that',\n",
       " 'ch',\n",
       " '##rom',\n",
       " '##o',\n",
       " 'domains',\n",
       " 'may',\n",
       " 'function',\n",
       " 'as',\n",
       " 'protein',\n",
       " 'interaction',\n",
       " 'motifs',\n",
       " ',',\n",
       " 'bringing',\n",
       " 'together',\n",
       " 'different',\n",
       " 'proteins',\n",
       " 'in',\n",
       " 'multi',\n",
       " '-',\n",
       " 'protein',\n",
       " 'complexes',\n",
       " 'and',\n",
       " 'locating',\n",
       " 'them',\n",
       " 'in',\n",
       " 'het',\n",
       " '##ero',\n",
       " '##ch',\n",
       " '##rom',\n",
       " '##atin',\n",
       " '.',\n",
       " 'we',\n",
       " 'have',\n",
       " 'previously',\n",
       " 'determined',\n",
       " 'the',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ch',\n",
       " '##rom',\n",
       " '##o',\n",
       " 'domain',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mouse',\n",
       " 'hp',\n",
       " '##1',\n",
       " '##bet',\n",
       " '##a',\n",
       " 'protein',\n",
       " ',',\n",
       " 'mod',\n",
       " '##1',\n",
       " '.',\n",
       " 'we',\n",
       " 'show',\n",
       " 'here',\n",
       " 'that',\n",
       " ',',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'the',\n",
       " 'ch',\n",
       " '##rom',\n",
       " '##o',\n",
       " 'domain',\n",
       " ',',\n",
       " 'the',\n",
       " 'shadow',\n",
       " 'ch',\n",
       " '##rom',\n",
       " '##o',\n",
       " 'domain',\n",
       " 'is',\n",
       " 'a',\n",
       " 'homo',\n",
       " '##dim',\n",
       " '##er',\n",
       " '.',\n",
       " 'the',\n",
       " 'intact',\n",
       " 'hp',\n",
       " '##1',\n",
       " '##bet',\n",
       " '##a',\n",
       " 'protein',\n",
       " 'is',\n",
       " 'also',\n",
       " 'dime',\n",
       " '##ric',\n",
       " ',',\n",
       " 'where',\n",
       " 'the',\n",
       " 'interaction',\n",
       " 'is',\n",
       " 'mediated',\n",
       " 'by',\n",
       " 'the',\n",
       " 'shadow',\n",
       " 'ch',\n",
       " '##rom',\n",
       " '##o',\n",
       " 'domain',\n",
       " ',',\n",
       " 'with',\n",
       " 'the',\n",
       " 'ch',\n",
       " '##rom',\n",
       " '##o',\n",
       " 'domains',\n",
       " 'moving',\n",
       " 'independently',\n",
       " 'of',\n",
       " 'each',\n",
       " 'other',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'of',\n",
       " 'flexible',\n",
       " 'link',\n",
       " '##ers',\n",
       " '.',\n",
       " 'mapping',\n",
       " 'studies',\n",
       " ',',\n",
       " 'with',\n",
       " 'fragments',\n",
       " 'of',\n",
       " 'the',\n",
       " 'caf',\n",
       " '##1',\n",
       " 'and',\n",
       " 'ti',\n",
       " '##f',\n",
       " '##1',\n",
       " '##bet',\n",
       " '##a',\n",
       " 'proteins',\n",
       " ',',\n",
       " 'show',\n",
       " 'that',\n",
       " 'an',\n",
       " 'intact',\n",
       " ',',\n",
       " 'dime',\n",
       " '##ric',\n",
       " ',',\n",
       " 'shadow',\n",
       " 'ch',\n",
       " '##rom',\n",
       " '##o',\n",
       " 'domain',\n",
       " 'structure',\n",
       " 'is',\n",
       " 'required',\n",
       " 'for',\n",
       " 'complex',\n",
       " 'formation',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_config.tokenizer.convert_ids_to_tokens(x_val[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_idx (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segments (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 512, 768), ( 109482240   input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 input_segments[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          76900       global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            101         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 109,559,241\n",
      "Trainable params: 109,559,241\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from mlearning.dl_models import Bert_Dense_opt\n",
    "bert_name = \"bert-base-uncased\"\n",
    "if multiple_gpus:\n",
    "    with strategy.scope():\n",
    "        model = Bert_Dense(dl_config, learning_rate=dl_config.learning_rate,static_bert=False, bert_name_or_path=bert_name)\n",
    "        #model = Bert_FE(dl_config, learning_rate=dl_config.learning_rate, bert_name_or_path=bert_name)\n",
    "        #model = Bert_Sequence_FT(dl_config, learning_rate=dl_config.learning_rate, bert_name_or_path=bert_name)\n",
    "        #model = Bert_Sequence_FE(dl_config, learning_rate=dl_config.learning_rate, bert_name_or_path=bert_name)\n",
    "        \n",
    "else:\n",
    "    model = Bert_Dense(dl_config, learning_rate=dl_config.learning_rate, static_bert=False, bert_name_or_path=bert_name)\n",
    "    #model = Bert_FE(dl_config, learning_rate=dl_config.learning_rate, bert_name_or_path=bert_name)\n",
    "    #model = Bert_Sequence_FT(dl_config, learning_rate=dl_config.learning_rate, bert_name_or_path=bert_name)\n",
    "    #model = Bert_Sequence_FE(dl_config, learning_rate=dl_config.learning_rate, bert_name_or_path=bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:batch_all_reduce: 198 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:batch_all_reduce: 198 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 3 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "230/230 [==============================] - ETA: 0s - accuracy: 0.6731 - loss: 0.5913INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "230/230 [==============================] - 152s 661ms/step - accuracy: 0.6731 - loss: 0.5913 - val_accuracy: 0.7304 - val_loss: 0.5295\n",
      "Epoch 2/3\n",
      "230/230 [==============================] - 142s 619ms/step - accuracy: 0.7695 - loss: 0.4807 - val_accuracy: 0.7647 - val_loss: 0.4590\n",
      "Epoch 3/3\n",
      "230/230 [==============================] - 143s 621ms/step - accuracy: 0.8323 - loss: 0.3692 - val_accuracy: 0.8015 - val_loss: 0.4692\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=dl_config.epochs,\n",
    "                    batch_size=dl_config.batch_size,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=keras_callbacks)\n",
    "\n",
    "if dl_config.keras_callbacks:\n",
    "    model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_loss, dl_config.train_acc = model.evaluate(x_train, y_train, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Training Loss: %.3f' % (train_loss))\n",
    "#print('Training Accuracy: %.3f' % (dl_config.train_acc))\n",
    "\n",
    "\n",
    "plot_training_history(history_dict = history, dl_config=dl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test = bioc_to_docs(test_dataset_path, dl_config=dl_config)\n",
    "relevances_test = bioc_to_relevances(test_dataset_path, 'protein-protein')\n",
    "\n",
    "x_test_df = docs_to_pandasdocs(docs_test)\n",
    "y_test_df = relevances_to_pandas(x_test_df, relevances_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = Bert_preprocessing(x_test_df, y_test_df, dl_config,\n",
    "                                    nmr_sentences=dl_config.nmr_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = model.predict(x_test, verbose=0)\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "\n",
    "yhat_classes = np.where(yhat_probs > 0.5, 1, yhat_probs)\n",
    "yhat_classes = np.where(yhat_classes < 0.5, 0, yhat_classes).astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC and Precision-Recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_config.test_roc_auc, dl_config.test_pr_auc = plot_roc_n_pr_curves(y_test, yhat_probs,dl_config = dl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dl_config.test_avg_prec  = average_precision(y_test_df, yhat_probs)\n",
    "print('Average Precision: %f' % dl_config.test_avg_prec)\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "dl_config.test_acc = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % dl_config.test_acc)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "dl_config.test_prec = precision_score(y_test, yhat_classes)\n",
    "print('Precision: %f' % dl_config.test_prec)\n",
    "\n",
    "# recall: tp / (tp + fn)\n",
    "dl_config.test_recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %f' % dl_config.test_recall)\n",
    "\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "dl_config.test_f1_score = f1_score(y_test, yhat_classes)\n",
    "print('F1 score: %f' % dl_config.test_f1_score)\n",
    "\n",
    "# ROC AUC\n",
    "print('ROC AUC: %f' % dl_config.test_roc_auc)\n",
    "\n",
    "# PR AUC\n",
    "print('PR AUC: %f' % dl_config.test_pr_auc)\n",
    "\n",
    "# kappa\n",
    "dl_config.test_kappa = cohen_kappa_score(y_test, yhat_classes)\n",
    "print('Cohens kappa: %f' % dl_config.test_kappa)\n",
    "\n",
    "dl_config.test_mcc = matthews_corrcoef(y_test, yhat_classes)\n",
    "print('MCC: %f' % dl_config.test_mcc)\n",
    "\n",
    "# confusion matrix\n",
    "matrix = confusion_matrix(y_test, yhat_classes)\n",
    "print('Confusion Matrix:\\n %s \\n' % matrix)\n",
    "\n",
    "dl_config.test_true_neg, dl_config.test_false_pos, dl_config.test_false_neg, dl_config.test_true_pos = confusion_matrix(\n",
    "                                                                                                y_test, yhat_classes).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_config.model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save DL_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_config.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_config.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_config.write_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(dl_config.model_id_path / 'model_tf', save_format = 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import Audio\n",
    "import numpy as np\n",
    "\n",
    "framerate = 4410\n",
    "play_time_seconds = 1\n",
    "\n",
    "t = np.linspace(0, play_time_seconds, framerate*play_time_seconds)\n",
    "audio_data = np.sin(2*np.pi*300*t) + np.sin(2*np.pi*240*t)\n",
    "Audio(audio_data, rate=framerate, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
