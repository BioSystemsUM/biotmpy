{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= 'hp_bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: biotmpygpu\n",
      "Num GPUs Available:  4\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "import os\n",
    "import tensorflow \n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "global seed_value\n",
    "seed_value = 123123\n",
    "#seed_value = None\n",
    "\n",
    "environment_name = sys.executable.split('/')[-3]\n",
    "print('Environment:', environment_name)\n",
    "os.environ[environment_name] = str(seed_value)\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "tensorflow.random.set_seed(seed_value)\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "multiple_gpus = [0,1,2,3]\n",
    "#multiple_gpus = None\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/malves/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/malves/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/malves/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "if multiple_gpus:\n",
    "    devices = []\n",
    "    for gpu in multiple_gpus:\n",
    "        devices.append('/gpu:' + str(gpu))    \n",
    "    strategy = tensorflow.distribute.MirroredStrategy(devices=devices)\n",
    "\n",
    "else:\n",
    "    # Get the GPU device name.\n",
    "    device_name = tensorflow.test.gpu_device_name()\n",
    "    # The device name should look like the following:\n",
    "    if device_name == '/device:GPU:0':\n",
    "        print('Using GPU: {}'.format(device_name))\n",
    "    else:\n",
    "        raise SystemError('GPU device not found')\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = device_name\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "from wrappers.bioc_wrapper import bioc_to_docs, bioc_to_relevances\n",
    "from wrappers.pandas_wrapper import relevances_to_pandas, docs_to_pandasdocs\n",
    "from mlearning.dl import DL_preprocessing\n",
    "from mlearning.dl_models import Hierarchical_Attention_GRU, Hierarchical_Attention_LSTM,Hierarchical_Attention_LSTM2, Hierarchical_Attention_LSTM3\n",
    "from mlearning.dl_models import Hierarchical_Attention_Context\n",
    "from mlearning.dl_models import DeepDTA\n",
    "from mlearning.embeddings import compute_embedding_matrix, glove_embeddings_2\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score, auc, roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from mlearning.dl import plot_training_history\n",
    "from mlearning.config import DLConfig\n",
    "from mlearning.dl import average_precision\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from mlearning.dl import plot_roc_n_pr_curves\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, LSTM, RNN, Bidirectional, Flatten, Activation, \\\n",
    "    RepeatVector, Permute, Multiply, Lambda, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlearning.attention import AttentionLayer\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import TimeDistributed, GRU\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import RMSprop, Adagrad, Adam, SGD\n",
    "from transformers import TFBertModel\n",
    "from mlearning.attention_context import AttentionWithContext\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from transformers import TFBertForSequenceClassification, BertConfig\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from kerastuner import Hyperband\n",
    "from transformers import BertTokenizer\n",
    "from mlearning.dl import Bert_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = '../datasets/PMtask_Triage_TrainingSet.xml'\n",
    "test_dataset_path = '../datasets/PMtask_Triage_TestSet.xml'\n",
    "\n",
    "\n",
    "\n",
    "config = DLConfig(model_name=model_name, seed_value=seed_value)\n",
    "#config.stop_words = set(stopwords.words('english'))           \n",
    "config.stop_words = None\n",
    "config.lower = True               \n",
    "config.remove_punctuation = False\n",
    "config.split_by_hyphen = False\n",
    "config.lemmatization = False           \n",
    "config.stems = False                      \n",
    "\n",
    "\n",
    "docs_train = bioc_to_docs(train_dataset_path, config=config)\n",
    "relevances_train = bioc_to_relevances(train_dataset_path, 'protein-protein')\n",
    "\n",
    "\n",
    "x_train_df = docs_to_pandasdocs(docs_train)\n",
    "y_train_df = relevances_to_pandas(x_train_df, relevances_train)\n",
    "\n",
    "#Parameters\n",
    "config.padding = 'post'            #'pre' -> default; 'post' -> alternative\n",
    "config.truncating = 'post'         #'pre' -> default; 'post' -> alternative      #####\n",
    "\n",
    "config.max_sent_len = 512      #sentences will have a maximum of \"max_sent_len\" words\n",
    "config.nmr_sentences = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with 3674 samples\n",
      "Validation set with 408 samples\n"
     ]
    }
   ],
   "source": [
    "config.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "x_train, y_train, x_val, y_val = Bert_preprocessing(x_train_df, y_train_df,\n",
    "    config=config,\n",
    "    validation_percentage=10,\n",
    "    seed_value=config.seed_value,\n",
    "    nmr_sentences = config.nmr_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_sent_len\n",
    "max_sent_len = config.max_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bert_hyper(hp):\n",
    "    idx = Input((max_sent_len), dtype=\"int32\", name=\"input_idx\")\n",
    "    masks = Input((max_sent_len), dtype=\"int32\", name=\"input_masks\")\n",
    "    segments = Input((max_sent_len), dtype=\"int32\", name=\"input_segments\")\n",
    "    \n",
    "    ## pre-trained bert\n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    embedding = bert_model([idx, masks, segments])[0]\n",
    "    #cls_token = embedding[:,0,:]\n",
    "    ## fine-tuning\n",
    "    #x = GlobalAveragePooling1D()(embedding)\n",
    "    #x = Dense(hp.Choice('dense_units', values=[64,128,256,512], default=128), activation=\"relu\")(x)\n",
    "    x = Bidirectional(LSTM(hp.Choice('lstm2_units', values=[64,128,256,512], default=128), return_sequences=True,  recurrent_dropout=0.1))(embedding)\n",
    "    x = Dropout(rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, default=0.2, step=0.1), seed=seed_value)\n",
    "    x = GlobalAveragePooling1D()(embedding)\n",
    "    x = Dense(hp.Choice('dense_units', values=[64,128,256], default=128), activation=\"relu\")(x)\n",
    "    x = Dropout(rate=hp.Float('dropout_2', min_value=0.0, max_value=0.5, default=0.2, step=0.1), seed=seed_value)(x)\n",
    "\n",
    "    y_out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model([idx, masks, segments], y_out)\n",
    "                           \n",
    "    optimizer_value = hp.Choice('optimizer_name', values=[ 'adagrad', 'rmsprop','adam', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', [5e-5, 3e-5, 2e-5])\n",
    "\n",
    "    if optimizer_value=='adagrad':\n",
    "        optimizer=Adagrad(lr=learning_rate)\n",
    "    elif optimizer_value=='adam':\n",
    "        optimizer=Adam(lr=learning_rate)\n",
    "    elif optimizer_value=='rmsprop':\n",
    "        optimizer=RMSprop(lr=learning_rate)\n",
    "    elif optimizer_value=='sgd':\n",
    "        optimizer=SGD(lr=learning_rate)\n",
    "\n",
    "        \n",
    "    for layer in model.layers[:4]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "        \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_idx (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segments (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 512, 768), ( 109482240   input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 input_segments[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          98432       global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            129         dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 109,580,801\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_idx (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segments (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 512, 768), ( 109482240   input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 input_segments[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 109,679,361\n",
      "Trainable params: 197,121\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.7393 - accuracy: 0.40 - ETA: 1:19 - loss: 0.6998 - accuracy: 0.50 - ETA: 1:45 - loss: 0.6929 - accuracy: 0.56 - ETA: 1:57 - loss: 0.6801 - accuracy: 0.57 - ETA: 2:04 - loss: 0.6977 - accuracy: 0.53 - ETA: 2:08 - loss: 0.7057 - accuracy: 0.49 - ETA: 2:11 - loss: 0.7056 - accuracy: 0.49 - ETA: 2:13 - loss: 0.7114 - accuracy: 0.48 - ETA: 2:14 - loss: 0.7159 - accuracy: 0.48 - ETA: 2:15 - loss: 0.7128 - accuracy: 0.49 - ETA: 2:15 - loss: 0.7118 - accuracy: 0.49 - ETA: 2:15 - loss: 0.7096 - accuracy: 0.49 - ETA: 2:14 - loss: 0.7075 - accuracy: 0.49 - ETA: 2:14 - loss: 0.7072 - accuracy: 0.50 - ETA: 2:13 - loss: 0.7016 - accuracy: 0.52 - ETA: 2:13 - loss: 0.7007 - accuracy: 0.52 - ETA: 2:12 - loss: 0.6974 - accuracy: 0.52 - ETA: 2:11 - loss: 0.6988 - accuracy: 0.52 - ETA: 2:10 - loss: 0.6969 - accuracy: 0.52 - ETA: 2:09 - loss: 0.6966 - accuracy: 0.52 - ETA: 2:08 - loss: 0.6948 - accuracy: 0.53 - ETA: 2:07 - loss: 0.6943 - accuracy: 0.53 - ETA: 2:06 - loss: 0.6944 - accuracy: 0.53 - ETA: 2:05 - loss: 0.6928 - accuracy: 0.53 - ETA: 2:04 - loss: 0.6908 - accuracy: 0.53 - ETA: 2:03 - loss: 0.6904 - accuracy: 0.53 - ETA: 2:02 - loss: 0.6919 - accuracy: 0.53 - ETA: 2:01 - loss: 0.6926 - accuracy: 0.53 - ETA: 1:59 - loss: 0.6938 - accuracy: 0.53 - ETA: 1:58 - loss: 0.6915 - accuracy: 0.54 - ETA: 1:57 - loss: 0.6917 - accuracy: 0.54 - ETA: 1:56 - loss: 0.6905 - accuracy: 0.54 - ETA: 1:54 - loss: 0.6885 - accuracy: 0.54 - ETA: 1:53 - loss: 0.6887 - accuracy: 0.55 - ETA: 1:52 - loss: 0.6886 - accuracy: 0.54 - ETA: 1:51 - loss: 0.6886 - accuracy: 0.55 - ETA: 1:49 - loss: 0.6878 - accuracy: 0.55 - ETA: 1:48 - loss: 0.6875 - accuracy: 0.55 - ETA: 1:47 - loss: 0.6879 - accuracy: 0.55 - ETA: 1:45 - loss: 0.6871 - accuracy: 0.55 - ETA: 1:44 - loss: 0.6867 - accuracy: 0.55 - ETA: 1:43 - loss: 0.6860 - accuracy: 0.55 - ETA: 1:41 - loss: 0.6866 - accuracy: 0.55 - ETA: 1:40 - loss: 0.6891 - accuracy: 0.54 - ETA: 1:39 - loss: 0.6887 - accuracy: 0.55 - ETA: 1:37 - loss: 0.6893 - accuracy: 0.54 - ETA: 1:36 - loss: 0.6896 - accuracy: 0.54 - ETA: 1:35 - loss: 0.6890 - accuracy: 0.54 - ETA: 1:33 - loss: 0.6895 - accuracy: 0.54 - ETA: 1:32 - loss: 0.6889 - accuracy: 0.54 - ETA: 1:31 - loss: 0.6887 - accuracy: 0.54 - ETA: 1:29 - loss: 0.6886 - accuracy: 0.54 - ETA: 1:28 - loss: 0.6889 - accuracy: 0.55 - ETA: 1:27 - loss: 0.6889 - accuracy: 0.54 - ETA: 1:25 - loss: 0.6895 - accuracy: 0.54 - ETA: 1:24 - loss: 0.6901 - accuracy: 0.54 - ETA: 1:23 - loss: 0.6896 - accuracy: 0.54 - ETA: 1:21 - loss: 0.6887 - accuracy: 0.54 - ETA: 1:20 - loss: 0.6889 - accuracy: 0.54 - ETA: 1:18 - loss: 0.6887 - accuracy: 0.55 - ETA: 1:17 - loss: 0.6881 - accuracy: 0.55 - ETA: 1:16 - loss: 0.6882 - accuracy: 0.55 - ETA: 1:14 - loss: 0.6883 - accuracy: 0.55 - ETA: 1:13 - loss: 0.6876 - accuracy: 0.55 - ETA: 1:11 - loss: 0.6880 - accuracy: 0.55 - ETA: 1:10 - loss: 0.6870 - accuracy: 0.55 - ETA: 1:09 - loss: 0.6870 - accuracy: 0.55 - ETA: 1:07 - loss: 0.6877 - accuracy: 0.55 - ETA: 1:06 - loss: 0.6886 - accuracy: 0.55 - ETA: 1:04 - loss: 0.6877 - accuracy: 0.55 - ETA: 1:03 - loss: 0.6869 - accuracy: 0.55 - ETA: 1:02 - loss: 0.6860 - accuracy: 0.55 - ETA: 1:00 - loss: 0.6856 - accuracy: 0.55 - ETA: 59s - loss: 0.6867 - accuracy: 0.5553 - ETA: 57s - loss: 0.6854 - accuracy: 0.557 - ETA: 56s - loss: 0.6854 - accuracy: 0.558 - ETA: 54s - loss: 0.6855 - accuracy: 0.556 - ETA: 53s - loss: 0.6854 - accuracy: 0.556 - ETA: 52s - loss: 0.6842 - accuracy: 0.558 - ETA: 50s - loss: 0.6851 - accuracy: 0.557 - ETA: 49s - loss: 0.6854 - accuracy: 0.555 - ETA: 47s - loss: 0.6858 - accuracy: 0.554 - ETA: 46s - loss: 0.6861 - accuracy: 0.554 - ETA: 44s - loss: 0.6859 - accuracy: 0.554 - ETA: 43s - loss: 0.6861 - accuracy: 0.554 - ETA: 42s - loss: 0.6860 - accuracy: 0.555 - ETA: 40s - loss: 0.6868 - accuracy: 0.553 - ETA: 39s - loss: 0.6866 - accuracy: 0.554 - ETA: 37s - loss: 0.6860 - accuracy: 0.555 - ETA: 36s - loss: 0.6866 - accuracy: 0.553 - ETA: 34s - loss: 0.6865 - accuracy: 0.553 - ETA: 33s - loss: 0.6861 - accuracy: 0.553 - ETA: 31s - loss: 0.6861 - accuracy: 0.553 - ETA: 30s - loss: 0.6858 - accuracy: 0.553 - ETA: 29s - loss: 0.6858 - accuracy: 0.553 - ETA: 27s - loss: 0.6853 - accuracy: 0.555 - ETA: 26s - loss: 0.6846 - accuracy: 0.556 - ETA: 24s - loss: 0.6843 - accuracy: 0.557 - ETA: 23s - loss: 0.6842 - accuracy: 0.558 - ETA: 21s - loss: 0.6845 - accuracy: 0.558 - ETA: 20s - loss: 0.6846 - accuracy: 0.557 - ETA: 18s - loss: 0.6843 - accuracy: 0.557 - ETA: 17s - loss: 0.6839 - accuracy: 0.558 - ETA: 16s - loss: 0.6838 - accuracy: 0.560 - ETA: 14s - loss: 0.6842 - accuracy: 0.559 - ETA: 13s - loss: 0.6842 - accuracy: 0.559 - ETA: 11s - loss: 0.6842 - accuracy: 0.559 - ETA: 10s - loss: 0.6841 - accuracy: 0.559 - ETA: 8s - loss: 0.6847 - accuracy: 0.559 - ETA: 7s - loss: 0.6846 - accuracy: 0.55 - ETA: 5s - loss: 0.6844 - accuracy: 0.55 - ETA: 4s - loss: 0.6841 - accuracy: 0.56 - ETA: 2s - loss: 0.6838 - accuracy: 0.56 - ETA: 1s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6833 - accuracy: 0.56 - 187s 2s/step - loss: 0.6833 - accuracy: 0.5610 - val_loss: 0.6576 - val_accuracy: 0.6103\n",
      "Epoch 2/2\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.7092 - accuracy: 0.53 - ETA: 1:24 - loss: 0.7015 - accuracy: 0.51 - ETA: 1:51 - loss: 0.7153 - accuracy: 0.51 - ETA: 2:05 - loss: 0.7035 - accuracy: 0.53 - ETA: 2:12 - loss: 0.7147 - accuracy: 0.52 - ETA: 2:15 - loss: 0.7043 - accuracy: 0.55 - ETA: 2:18 - loss: 0.6933 - accuracy: 0.56 - ETA: 2:19 - loss: 0.6946 - accuracy: 0.55 - ETA: 2:20 - loss: 0.6948 - accuracy: 0.55 - ETA: 2:20 - loss: 0.6938 - accuracy: 0.56 - ETA: 2:20 - loss: 0.6949 - accuracy: 0.55 - ETA: 2:20 - loss: 0.6963 - accuracy: 0.55 - ETA: 2:20 - loss: 0.7018 - accuracy: 0.54 - ETA: 2:19 - loss: 0.6988 - accuracy: 0.54 - ETA: 2:19 - loss: 0.6966 - accuracy: 0.54 - ETA: 2:18 - loss: 0.6951 - accuracy: 0.54 - ETA: 2:17 - loss: 0.6880 - accuracy: 0.55 - ETA: 2:16 - loss: 0.6870 - accuracy: 0.55 - ETA: 2:15 - loss: 0.6887 - accuracy: 0.55 - ETA: 2:14 - loss: 0.6881 - accuracy: 0.55 - ETA: 2:13 - loss: 0.6873 - accuracy: 0.56 - ETA: 2:12 - loss: 0.6867 - accuracy: 0.56 - ETA: 2:10 - loss: 0.6844 - accuracy: 0.56 - ETA: 2:09 - loss: 0.6840 - accuracy: 0.56 - ETA: 2:08 - loss: 0.6847 - accuracy: 0.56 - ETA: 2:07 - loss: 0.6829 - accuracy: 0.56 - ETA: 2:06 - loss: 0.6823 - accuracy: 0.57 - ETA: 2:04 - loss: 0.6812 - accuracy: 0.57 - ETA: 2:03 - loss: 0.6779 - accuracy: 0.57 - ETA: 2:02 - loss: 0.6778 - accuracy: 0.57 - ETA: 2:00 - loss: 0.6772 - accuracy: 0.57 - ETA: 1:59 - loss: 0.6750 - accuracy: 0.58 - ETA: 1:58 - loss: 0.6730 - accuracy: 0.58 - ETA: 1:56 - loss: 0.6737 - accuracy: 0.58 - ETA: 1:55 - loss: 0.6748 - accuracy: 0.58 - ETA: 1:54 - loss: 0.6741 - accuracy: 0.58 - ETA: 1:52 - loss: 0.6737 - accuracy: 0.58 - ETA: 1:51 - loss: 0.6748 - accuracy: 0.57 - ETA: 1:50 - loss: 0.6754 - accuracy: 0.57 - ETA: 1:48 - loss: 0.6739 - accuracy: 0.57 - ETA: 1:47 - loss: 0.6745 - accuracy: 0.57 - ETA: 1:45 - loss: 0.6737 - accuracy: 0.57 - ETA: 1:44 - loss: 0.6731 - accuracy: 0.58 - ETA: 1:43 - loss: 0.6730 - accuracy: 0.58 - ETA: 1:41 - loss: 0.6731 - accuracy: 0.58 - ETA: 1:40 - loss: 0.6730 - accuracy: 0.58 - ETA: 1:39 - loss: 0.6732 - accuracy: 0.58 - ETA: 1:37 - loss: 0.6731 - accuracy: 0.58 - ETA: 1:36 - loss: 0.6728 - accuracy: 0.58 - ETA: 1:34 - loss: 0.6739 - accuracy: 0.58 - ETA: 1:33 - loss: 0.6743 - accuracy: 0.58 - ETA: 1:31 - loss: 0.6742 - accuracy: 0.58 - ETA: 1:30 - loss: 0.6737 - accuracy: 0.58 - ETA: 1:29 - loss: 0.6738 - accuracy: 0.58 - ETA: 1:27 - loss: 0.6742 - accuracy: 0.58 - ETA: 1:26 - loss: 0.6749 - accuracy: 0.57 - ETA: 1:24 - loss: 0.6737 - accuracy: 0.58 - ETA: 1:23 - loss: 0.6737 - accuracy: 0.58 - ETA: 1:21 - loss: 0.6738 - accuracy: 0.58 - ETA: 1:20 - loss: 0.6735 - accuracy: 0.58 - ETA: 1:19 - loss: 0.6731 - accuracy: 0.58 - ETA: 1:17 - loss: 0.6735 - accuracy: 0.58 - ETA: 1:16 - loss: 0.6729 - accuracy: 0.58 - ETA: 1:14 - loss: 0.6729 - accuracy: 0.58 - ETA: 1:13 - loss: 0.6725 - accuracy: 0.58 - ETA: 1:11 - loss: 0.6728 - accuracy: 0.58 - ETA: 1:10 - loss: 0.6725 - accuracy: 0.58 - ETA: 1:08 - loss: 0.6722 - accuracy: 0.58 - ETA: 1:07 - loss: 0.6718 - accuracy: 0.58 - ETA: 1:06 - loss: 0.6715 - accuracy: 0.58 - ETA: 1:04 - loss: 0.6716 - accuracy: 0.58 - ETA: 1:03 - loss: 0.6715 - accuracy: 0.58 - ETA: 1:01 - loss: 0.6706 - accuracy: 0.58 - ETA: 1:00 - loss: 0.6705 - accuracy: 0.58 - ETA: 58s - loss: 0.6705 - accuracy: 0.5829 - ETA: 57s - loss: 0.6710 - accuracy: 0.582 - ETA: 55s - loss: 0.6712 - accuracy: 0.583 - ETA: 54s - loss: 0.6712 - accuracy: 0.584 - ETA: 52s - loss: 0.6705 - accuracy: 0.585 - ETA: 51s - loss: 0.6703 - accuracy: 0.586 - ETA: 49s - loss: 0.6701 - accuracy: 0.586 - ETA: 48s - loss: 0.6702 - accuracy: 0.586 - ETA: 47s - loss: 0.6699 - accuracy: 0.587 - ETA: 45s - loss: 0.6703 - accuracy: 0.585 - ETA: 44s - loss: 0.6706 - accuracy: 0.585 - ETA: 42s - loss: 0.6702 - accuracy: 0.585 - ETA: 41s - loss: 0.6703 - accuracy: 0.586 - ETA: 39s - loss: 0.6710 - accuracy: 0.585 - ETA: 38s - loss: 0.6710 - accuracy: 0.585 - ETA: 36s - loss: 0.6710 - accuracy: 0.584 - ETA: 35s - loss: 0.6699 - accuracy: 0.586 - ETA: 33s - loss: 0.6697 - accuracy: 0.587 - ETA: 32s - loss: 0.6696 - accuracy: 0.587 - ETA: 30s - loss: 0.6690 - accuracy: 0.587 - ETA: 29s - loss: 0.6677 - accuracy: 0.590 - ETA: 28s - loss: 0.6672 - accuracy: 0.592 - ETA: 26s - loss: 0.6675 - accuracy: 0.592 - ETA: 25s - loss: 0.6674 - accuracy: 0.593 - ETA: 23s - loss: 0.6673 - accuracy: 0.592 - ETA: 22s - loss: 0.6670 - accuracy: 0.593 - ETA: 20s - loss: 0.6674 - accuracy: 0.593 - ETA: 19s - loss: 0.6666 - accuracy: 0.594 - ETA: 17s - loss: 0.6659 - accuracy: 0.595 - ETA: 16s - loss: 0.6665 - accuracy: 0.595 - ETA: 14s - loss: 0.6672 - accuracy: 0.594 - ETA: 13s - loss: 0.6668 - accuracy: 0.595 - ETA: 11s - loss: 0.6673 - accuracy: 0.594 - ETA: 10s - loss: 0.6677 - accuracy: 0.592 - ETA: 8s - loss: 0.6673 - accuracy: 0.593 - ETA: 7s - loss: 0.6664 - accuracy: 0.59 - ETA: 5s - loss: 0.6660 - accuracy: 0.59 - ETA: 4s - loss: 0.6660 - accuracy: 0.59 - ETA: 2s - loss: 0.6661 - accuracy: 0.59 - ETA: 1s - loss: 0.6667 - accuracy: 0.59 - ETA: 0s - loss: 0.6666 - accuracy: 0.59 - 187s 2s/step - loss: 0.6666 - accuracy: 0.5942 - val_loss: 0.6486 - val_accuracy: 0.6225\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 17fc52a28638747129eb4629a2cc7c42</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.6225489974021912</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dense_units: 256</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dropout_1: 0.2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dropout_2: 0.4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-learning_rate: 3e-05</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-lstm2_units: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-optimizer_name: rmsprop</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/bracket: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/epochs: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/initial_epoch: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/round: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_idx (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segments (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 512, 768), ( 109482240   input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 input_segments[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 109,679,361\n",
      "Trainable params: 197,121\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.6719 - accuracy: 0.62 - ETA: 1:22 - loss: 0.6932 - accuracy: 0.56 - ETA: 1:48 - loss: 0.6975 - accuracy: 0.55 - ETA: 2:01 - loss: 0.6857 - accuracy: 0.57 - ETA: 2:08 - loss: 0.6979 - accuracy: 0.53 - ETA: 2:12 - loss: 0.7052 - accuracy: 0.52 - ETA: 2:15 - loss: 0.7111 - accuracy: 0.50 - ETA: 2:16 - loss: 0.7074 - accuracy: 0.51 - ETA: 2:17 - loss: 0.7009 - accuracy: 0.52 - ETA: 2:18 - loss: 0.7017 - accuracy: 0.51 - ETA: 2:18 - loss: 0.6999 - accuracy: 0.52 - ETA: 2:18 - loss: 0.7000 - accuracy: 0.52 - ETA: 2:17 - loss: 0.6976 - accuracy: 0.53 - ETA: 2:17 - loss: 0.6981 - accuracy: 0.52 - ETA: 2:16 - loss: 0.6979 - accuracy: 0.53 - ETA: 2:15 - loss: 0.7016 - accuracy: 0.53 - ETA: 2:15 - loss: 0.7017 - accuracy: 0.52 - ETA: 2:14 - loss: 0.7040 - accuracy: 0.52 - ETA: 2:13 - loss: 0.7004 - accuracy: 0.53 - ETA: 2:12 - loss: 0.6995 - accuracy: 0.53 - ETA: 2:11 - loss: 0.6980 - accuracy: 0.54 - ETA: 2:10 - loss: 0.6967 - accuracy: 0.54 - ETA: 2:09 - loss: 0.6971 - accuracy: 0.54 - ETA: 2:08 - loss: 0.6949 - accuracy: 0.54 - ETA: 2:07 - loss: 0.6918 - accuracy: 0.55 - ETA: 2:06 - loss: 0.6939 - accuracy: 0.55 - ETA: 2:04 - loss: 0.6935 - accuracy: 0.55 - ETA: 2:03 - loss: 0.6932 - accuracy: 0.55 - ETA: 2:02 - loss: 0.6919 - accuracy: 0.55 - ETA: 2:01 - loss: 0.6909 - accuracy: 0.55 - ETA: 1:59 - loss: 0.6901 - accuracy: 0.55 - ETA: 1:58 - loss: 0.6894 - accuracy: 0.56 - ETA: 1:57 - loss: 0.6883 - accuracy: 0.56 - ETA: 1:56 - loss: 0.6874 - accuracy: 0.56 - ETA: 1:54 - loss: 0.6883 - accuracy: 0.56 - ETA: 1:53 - loss: 0.6870 - accuracy: 0.56 - ETA: 1:52 - loss: 0.6872 - accuracy: 0.56 - ETA: 1:50 - loss: 0.6869 - accuracy: 0.56 - ETA: 1:49 - loss: 0.6868 - accuracy: 0.56 - ETA: 1:48 - loss: 0.6860 - accuracy: 0.56 - ETA: 1:46 - loss: 0.6861 - accuracy: 0.56 - ETA: 1:45 - loss: 0.6870 - accuracy: 0.56 - ETA: 1:43 - loss: 0.6864 - accuracy: 0.56 - ETA: 1:42 - loss: 0.6883 - accuracy: 0.56 - ETA: 1:41 - loss: 0.6895 - accuracy: 0.56 - ETA: 1:39 - loss: 0.6906 - accuracy: 0.55 - ETA: 1:38 - loss: 0.6910 - accuracy: 0.55 - ETA: 1:37 - loss: 0.6909 - accuracy: 0.56 - ETA: 1:35 - loss: 0.6911 - accuracy: 0.56 - ETA: 1:34 - loss: 0.6907 - accuracy: 0.56 - ETA: 1:32 - loss: 0.6903 - accuracy: 0.56 - ETA: 1:31 - loss: 0.6909 - accuracy: 0.56 - ETA: 1:30 - loss: 0.6914 - accuracy: 0.55 - ETA: 1:28 - loss: 0.6921 - accuracy: 0.55 - ETA: 1:27 - loss: 0.6919 - accuracy: 0.55 - ETA: 1:25 - loss: 0.6916 - accuracy: 0.55 - ETA: 1:24 - loss: 0.6906 - accuracy: 0.55 - ETA: 1:22 - loss: 0.6898 - accuracy: 0.55 - ETA: 1:21 - loss: 0.6896 - accuracy: 0.55 - ETA: 1:20 - loss: 0.6894 - accuracy: 0.55 - ETA: 1:18 - loss: 0.6889 - accuracy: 0.56 - ETA: 1:17 - loss: 0.6893 - accuracy: 0.55 - ETA: 1:15 - loss: 0.6890 - accuracy: 0.55 - ETA: 1:14 - loss: 0.6887 - accuracy: 0.55 - ETA: 1:12 - loss: 0.6885 - accuracy: 0.55 - ETA: 1:11 - loss: 0.6877 - accuracy: 0.56 - ETA: 1:10 - loss: 0.6873 - accuracy: 0.56 - ETA: 1:08 - loss: 0.6881 - accuracy: 0.56 - ETA: 1:07 - loss: 0.6885 - accuracy: 0.55 - ETA: 1:05 - loss: 0.6888 - accuracy: 0.55 - ETA: 1:04 - loss: 0.6883 - accuracy: 0.56 - ETA: 1:02 - loss: 0.6874 - accuracy: 0.56 - ETA: 1:01 - loss: 0.6875 - accuracy: 0.56 - ETA: 1:00 - loss: 0.6879 - accuracy: 0.56 - ETA: 58s - loss: 0.6880 - accuracy: 0.5608 - ETA: 57s - loss: 0.6879 - accuracy: 0.560 - ETA: 55s - loss: 0.6883 - accuracy: 0.559 - ETA: 54s - loss: 0.6882 - accuracy: 0.558 - ETA: 52s - loss: 0.6881 - accuracy: 0.559 - ETA: 51s - loss: 0.6884 - accuracy: 0.558 - ETA: 49s - loss: 0.6886 - accuracy: 0.557 - ETA: 48s - loss: 0.6885 - accuracy: 0.558 - ETA: 46s - loss: 0.6883 - accuracy: 0.559 - ETA: 45s - loss: 0.6879 - accuracy: 0.560 - ETA: 44s - loss: 0.6884 - accuracy: 0.560 - ETA: 42s - loss: 0.6887 - accuracy: 0.559 - ETA: 41s - loss: 0.6890 - accuracy: 0.557 - ETA: 39s - loss: 0.6888 - accuracy: 0.558 - ETA: 38s - loss: 0.6881 - accuracy: 0.558 - ETA: 36s - loss: 0.6880 - accuracy: 0.558 - ETA: 35s - loss: 0.6874 - accuracy: 0.559 - ETA: 33s - loss: 0.6876 - accuracy: 0.557 - ETA: 32s - loss: 0.6880 - accuracy: 0.556 - ETA: 30s - loss: 0.6877 - accuracy: 0.556 - ETA: 29s - loss: 0.6875 - accuracy: 0.556 - ETA: 27s - loss: 0.6874 - accuracy: 0.557 - ETA: 26s - loss: 0.6870 - accuracy: 0.558 - ETA: 24s - loss: 0.6867 - accuracy: 0.558 - ETA: 23s - loss: 0.6866 - accuracy: 0.558 - ETA: 22s - loss: 0.6868 - accuracy: 0.558 - ETA: 20s - loss: 0.6872 - accuracy: 0.557 - ETA: 19s - loss: 0.6872 - accuracy: 0.557 - ETA: 17s - loss: 0.6871 - accuracy: 0.557 - ETA: 16s - loss: 0.6869 - accuracy: 0.558 - ETA: 14s - loss: 0.6870 - accuracy: 0.558 - ETA: 13s - loss: 0.6864 - accuracy: 0.560 - ETA: 11s - loss: 0.6866 - accuracy: 0.560 - ETA: 10s - loss: 0.6864 - accuracy: 0.561 - ETA: 8s - loss: 0.6864 - accuracy: 0.561 - ETA: 7s - loss: 0.6862 - accuracy: 0.56 - ETA: 5s - loss: 0.6856 - accuracy: 0.56 - ETA: 4s - loss: 0.6855 - accuracy: 0.56 - ETA: 2s - loss: 0.6852 - accuracy: 0.56 - ETA: 1s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6847 - accuracy: 0.56 - 191s 2s/step - loss: 0.6847 - accuracy: 0.5651 - val_loss: 0.6661 - val_accuracy: 0.5907\n",
      "Epoch 2/2\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.6795 - accuracy: 0.62 - ETA: 1:23 - loss: 0.6998 - accuracy: 0.56 - ETA: 1:50 - loss: 0.6921 - accuracy: 0.58 - ETA: 2:03 - loss: 0.6786 - accuracy: 0.59 - ETA: 2:10 - loss: 0.6796 - accuracy: 0.57 - ETA: 2:14 - loss: 0.6796 - accuracy: 0.57 - ETA: 2:17 - loss: 0.6749 - accuracy: 0.57 - ETA: 2:18 - loss: 0.6737 - accuracy: 0.57 - ETA: 2:19 - loss: 0.6766 - accuracy: 0.55 - ETA: 2:19 - loss: 0.6745 - accuracy: 0.56 - ETA: 2:19 - loss: 0.6774 - accuracy: 0.56 - ETA: 2:19 - loss: 0.6847 - accuracy: 0.54 - ETA: 2:19 - loss: 0.6846 - accuracy: 0.54 - ETA: 2:19 - loss: 0.6825 - accuracy: 0.54 - ETA: 2:18 - loss: 0.6817 - accuracy: 0.55 - ETA: 2:18 - loss: 0.6806 - accuracy: 0.55 - ETA: 2:17 - loss: 0.6769 - accuracy: 0.56 - ETA: 2:16 - loss: 0.6750 - accuracy: 0.57 - ETA: 2:15 - loss: 0.6751 - accuracy: 0.57 - ETA: 2:14 - loss: 0.6765 - accuracy: 0.56 - ETA: 2:13 - loss: 0.6757 - accuracy: 0.56 - ETA: 2:12 - loss: 0.6736 - accuracy: 0.57 - ETA: 2:10 - loss: 0.6725 - accuracy: 0.57 - ETA: 2:09 - loss: 0.6714 - accuracy: 0.57 - ETA: 2:08 - loss: 0.6711 - accuracy: 0.57 - ETA: 2:07 - loss: 0.6714 - accuracy: 0.57 - ETA: 2:06 - loss: 0.6719 - accuracy: 0.57 - ETA: 2:04 - loss: 0.6715 - accuracy: 0.57 - ETA: 2:03 - loss: 0.6703 - accuracy: 0.57 - ETA: 2:02 - loss: 0.6706 - accuracy: 0.57 - ETA: 2:01 - loss: 0.6716 - accuracy: 0.57 - ETA: 1:59 - loss: 0.6706 - accuracy: 0.58 - ETA: 1:58 - loss: 0.6699 - accuracy: 0.58 - ETA: 1:56 - loss: 0.6708 - accuracy: 0.58 - ETA: 1:55 - loss: 0.6725 - accuracy: 0.57 - ETA: 1:54 - loss: 0.6724 - accuracy: 0.57 - ETA: 1:52 - loss: 0.6724 - accuracy: 0.57 - ETA: 1:51 - loss: 0.6735 - accuracy: 0.57 - ETA: 1:50 - loss: 0.6745 - accuracy: 0.56 - ETA: 1:49 - loss: 0.6746 - accuracy: 0.57 - ETA: 1:47 - loss: 0.6744 - accuracy: 0.57 - ETA: 1:46 - loss: 0.6738 - accuracy: 0.57 - ETA: 1:44 - loss: 0.6733 - accuracy: 0.57 - ETA: 1:43 - loss: 0.6740 - accuracy: 0.57 - ETA: 1:41 - loss: 0.6746 - accuracy: 0.57 - ETA: 1:40 - loss: 0.6739 - accuracy: 0.57 - ETA: 1:39 - loss: 0.6743 - accuracy: 0.57 - ETA: 1:37 - loss: 0.6744 - accuracy: 0.57 - ETA: 1:36 - loss: 0.6744 - accuracy: 0.57 - ETA: 1:34 - loss: 0.6746 - accuracy: 0.57 - ETA: 1:33 - loss: 0.6750 - accuracy: 0.57 - ETA: 1:32 - loss: 0.6747 - accuracy: 0.57 - ETA: 1:30 - loss: 0.6745 - accuracy: 0.57 - ETA: 1:29 - loss: 0.6744 - accuracy: 0.57 - ETA: 1:27 - loss: 0.6745 - accuracy: 0.57 - ETA: 1:26 - loss: 0.6740 - accuracy: 0.57 - ETA: 1:24 - loss: 0.6732 - accuracy: 0.57 - ETA: 1:23 - loss: 0.6728 - accuracy: 0.57 - ETA: 1:22 - loss: 0.6730 - accuracy: 0.57 - ETA: 1:20 - loss: 0.6729 - accuracy: 0.57 - ETA: 1:19 - loss: 0.6735 - accuracy: 0.57 - ETA: 1:17 - loss: 0.6745 - accuracy: 0.57 - ETA: 1:16 - loss: 0.6742 - accuracy: 0.57 - ETA: 1:14 - loss: 0.6742 - accuracy: 0.57 - ETA: 1:13 - loss: 0.6743 - accuracy: 0.57 - ETA: 1:11 - loss: 0.6741 - accuracy: 0.57 - ETA: 1:10 - loss: 0.6743 - accuracy: 0.57 - ETA: 1:09 - loss: 0.6744 - accuracy: 0.57 - ETA: 1:07 - loss: 0.6745 - accuracy: 0.57 - ETA: 1:06 - loss: 0.6743 - accuracy: 0.57 - ETA: 1:04 - loss: 0.6745 - accuracy: 0.57 - ETA: 1:03 - loss: 0.6746 - accuracy: 0.57 - ETA: 1:01 - loss: 0.6745 - accuracy: 0.57 - ETA: 1:00 - loss: 0.6746 - accuracy: 0.57 - ETA: 58s - loss: 0.6745 - accuracy: 0.5742 - ETA: 57s - loss: 0.6750 - accuracy: 0.572 - ETA: 56s - loss: 0.6747 - accuracy: 0.572 - ETA: 54s - loss: 0.6744 - accuracy: 0.572 - ETA: 53s - loss: 0.6737 - accuracy: 0.574 - ETA: 51s - loss: 0.6735 - accuracy: 0.575 - ETA: 50s - loss: 0.6731 - accuracy: 0.576 - ETA: 48s - loss: 0.6726 - accuracy: 0.578 - ETA: 47s - loss: 0.6727 - accuracy: 0.578 - ETA: 45s - loss: 0.6729 - accuracy: 0.577 - ETA: 44s - loss: 0.6731 - accuracy: 0.577 - ETA: 42s - loss: 0.6723 - accuracy: 0.579 - ETA: 41s - loss: 0.6724 - accuracy: 0.578 - ETA: 39s - loss: 0.6723 - accuracy: 0.578 - ETA: 38s - loss: 0.6719 - accuracy: 0.578 - ETA: 36s - loss: 0.6719 - accuracy: 0.578 - ETA: 35s - loss: 0.6716 - accuracy: 0.579 - ETA: 33s - loss: 0.6715 - accuracy: 0.578 - ETA: 32s - loss: 0.6713 - accuracy: 0.579 - ETA: 31s - loss: 0.6706 - accuracy: 0.580 - ETA: 29s - loss: 0.6700 - accuracy: 0.581 - ETA: 28s - loss: 0.6698 - accuracy: 0.583 - ETA: 26s - loss: 0.6702 - accuracy: 0.582 - ETA: 25s - loss: 0.6704 - accuracy: 0.582 - ETA: 23s - loss: 0.6703 - accuracy: 0.582 - ETA: 22s - loss: 0.6701 - accuracy: 0.583 - ETA: 20s - loss: 0.6707 - accuracy: 0.582 - ETA: 19s - loss: 0.6699 - accuracy: 0.584 - ETA: 17s - loss: 0.6688 - accuracy: 0.586 - ETA: 16s - loss: 0.6690 - accuracy: 0.586 - ETA: 14s - loss: 0.6692 - accuracy: 0.586 - ETA: 13s - loss: 0.6690 - accuracy: 0.587 - ETA: 11s - loss: 0.6692 - accuracy: 0.587 - ETA: 10s - loss: 0.6697 - accuracy: 0.586 - ETA: 8s - loss: 0.6697 - accuracy: 0.585 - ETA: 7s - loss: 0.6691 - accuracy: 0.58 - ETA: 5s - loss: 0.6688 - accuracy: 0.58 - ETA: 4s - loss: 0.6690 - accuracy: 0.58 - ETA: 2s - loss: 0.6686 - accuracy: 0.58 - ETA: 1s - loss: 0.6687 - accuracy: 0.58 - ETA: 0s - loss: 0.6686 - accuracy: 0.58 - 187s 2s/step - loss: 0.6686 - accuracy: 0.5882 - val_loss: 0.6531 - val_accuracy: 0.6225\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 3f58a852205a3b52cc613ff41f138332</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.6225489974021912</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dense_units: 256</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dropout_1: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dropout_2: 0.30000000000000004</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-learning_rate: 2e-05</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-lstm2_units: 128</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-optimizer_name: adam</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/bracket: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/epochs: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/initial_epoch: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/round: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_idx (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segments (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 512, 768), ( 109482240   input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 input_segments[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           49216       global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            65          dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 109,531,521\n",
      "Trainable params: 49,281\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.6650 - accuracy: 0.68 - ETA: 1:22 - loss: 0.6831 - accuracy: 0.64 - ETA: 1:49 - loss: 0.6779 - accuracy: 0.65 - ETA: 2:01 - loss: 0.6913 - accuracy: 0.62 - ETA: 2:08 - loss: 0.6923 - accuracy: 0.60 - ETA: 2:12 - loss: 0.7017 - accuracy: 0.58 - ETA: 2:15 - loss: 0.7065 - accuracy: 0.57 - ETA: 2:17 - loss: 0.7042 - accuracy: 0.55 - ETA: 2:18 - loss: 0.7027 - accuracy: 0.55 - ETA: 2:18 - loss: 0.7005 - accuracy: 0.57 - ETA: 2:19 - loss: 0.7046 - accuracy: 0.55 - ETA: 2:18 - loss: 0.7072 - accuracy: 0.56 - ETA: 2:18 - loss: 0.7018 - accuracy: 0.56 - ETA: 2:18 - loss: 0.7043 - accuracy: 0.56 - ETA: 2:17 - loss: 0.7028 - accuracy: 0.56 - ETA: 2:16 - loss: 0.7077 - accuracy: 0.55 - ETA: 2:15 - loss: 0.7048 - accuracy: 0.56 - ETA: 2:15 - loss: 0.7058 - accuracy: 0.56 - ETA: 2:14 - loss: 0.7064 - accuracy: 0.55 - ETA: 2:13 - loss: 0.7062 - accuracy: 0.55 - ETA: 2:12 - loss: 0.7028 - accuracy: 0.56 - ETA: 2:10 - loss: 0.6962 - accuracy: 0.56 - ETA: 2:09 - loss: 0.6966 - accuracy: 0.56 - ETA: 2:08 - loss: 0.6997 - accuracy: 0.56 - ETA: 2:07 - loss: 0.6970 - accuracy: 0.56 - ETA: 2:06 - loss: 0.6949 - accuracy: 0.56 - ETA: 2:05 - loss: 0.6960 - accuracy: 0.56 - ETA: 2:03 - loss: 0.6976 - accuracy: 0.55 - ETA: 2:02 - loss: 0.6980 - accuracy: 0.55 - ETA: 2:01 - loss: 0.6963 - accuracy: 0.56 - ETA: 1:59 - loss: 0.6971 - accuracy: 0.55 - ETA: 1:58 - loss: 0.6983 - accuracy: 0.55 - ETA: 1:57 - loss: 0.6992 - accuracy: 0.55 - ETA: 1:56 - loss: 0.6986 - accuracy: 0.55 - ETA: 1:54 - loss: 0.7011 - accuracy: 0.55 - ETA: 1:53 - loss: 0.7008 - accuracy: 0.55 - ETA: 1:52 - loss: 0.7012 - accuracy: 0.55 - ETA: 1:50 - loss: 0.7007 - accuracy: 0.55 - ETA: 1:49 - loss: 0.7015 - accuracy: 0.55 - ETA: 1:48 - loss: 0.7006 - accuracy: 0.55 - ETA: 1:47 - loss: 0.7003 - accuracy: 0.55 - ETA: 1:45 - loss: 0.7013 - accuracy: 0.55 - ETA: 1:44 - loss: 0.7004 - accuracy: 0.55 - ETA: 1:42 - loss: 0.7039 - accuracy: 0.55 - ETA: 1:41 - loss: 0.7048 - accuracy: 0.54 - ETA: 1:40 - loss: 0.7069 - accuracy: 0.54 - ETA: 1:38 - loss: 0.7067 - accuracy: 0.54 - ETA: 1:37 - loss: 0.7062 - accuracy: 0.54 - ETA: 1:35 - loss: 0.7070 - accuracy: 0.54 - ETA: 1:34 - loss: 0.7073 - accuracy: 0.54 - ETA: 1:33 - loss: 0.7072 - accuracy: 0.54 - ETA: 1:31 - loss: 0.7079 - accuracy: 0.54 - ETA: 1:30 - loss: 0.7083 - accuracy: 0.54 - ETA: 1:28 - loss: 0.7099 - accuracy: 0.54 - ETA: 1:27 - loss: 0.7109 - accuracy: 0.54 - ETA: 1:25 - loss: 0.7136 - accuracy: 0.54 - ETA: 1:24 - loss: 0.7134 - accuracy: 0.54 - ETA: 1:23 - loss: 0.7119 - accuracy: 0.54 - ETA: 1:21 - loss: 0.7115 - accuracy: 0.54 - ETA: 1:20 - loss: 0.7121 - accuracy: 0.54 - ETA: 1:18 - loss: 0.7129 - accuracy: 0.54 - ETA: 1:17 - loss: 0.7129 - accuracy: 0.54 - ETA: 1:15 - loss: 0.7123 - accuracy: 0.54 - ETA: 1:14 - loss: 0.7115 - accuracy: 0.54 - ETA: 1:13 - loss: 0.7108 - accuracy: 0.54 - ETA: 1:11 - loss: 0.7103 - accuracy: 0.54 - ETA: 1:10 - loss: 0.7097 - accuracy: 0.54 - ETA: 1:08 - loss: 0.7095 - accuracy: 0.55 - ETA: 1:07 - loss: 0.7093 - accuracy: 0.55 - ETA: 1:05 - loss: 0.7101 - accuracy: 0.54 - ETA: 1:04 - loss: 0.7100 - accuracy: 0.54 - ETA: 1:02 - loss: 0.7088 - accuracy: 0.54 - ETA: 1:01 - loss: 0.7084 - accuracy: 0.54 - ETA: 1:00 - loss: 0.7092 - accuracy: 0.54 - ETA: 58s - loss: 0.7097 - accuracy: 0.5454 - ETA: 57s - loss: 0.7103 - accuracy: 0.545 - ETA: 55s - loss: 0.7110 - accuracy: 0.544 - ETA: 54s - loss: 0.7113 - accuracy: 0.544 - ETA: 52s - loss: 0.7106 - accuracy: 0.545 - ETA: 51s - loss: 0.7105 - accuracy: 0.544 - ETA: 49s - loss: 0.7113 - accuracy: 0.544 - ETA: 48s - loss: 0.7114 - accuracy: 0.543 - ETA: 46s - loss: 0.7118 - accuracy: 0.542 - ETA: 45s - loss: 0.7114 - accuracy: 0.544 - ETA: 44s - loss: 0.7112 - accuracy: 0.544 - ETA: 42s - loss: 0.7117 - accuracy: 0.543 - ETA: 41s - loss: 0.7112 - accuracy: 0.543 - ETA: 39s - loss: 0.7102 - accuracy: 0.545 - ETA: 38s - loss: 0.7102 - accuracy: 0.545 - ETA: 36s - loss: 0.7099 - accuracy: 0.545 - ETA: 35s - loss: 0.7097 - accuracy: 0.545 - ETA: 33s - loss: 0.7099 - accuracy: 0.546 - ETA: 32s - loss: 0.7104 - accuracy: 0.546 - ETA: 30s - loss: 0.7098 - accuracy: 0.546 - ETA: 29s - loss: 0.7102 - accuracy: 0.545 - ETA: 27s - loss: 0.7096 - accuracy: 0.546 - ETA: 26s - loss: 0.7089 - accuracy: 0.547 - ETA: 25s - loss: 0.7082 - accuracy: 0.548 - ETA: 23s - loss: 0.7085 - accuracy: 0.548 - ETA: 22s - loss: 0.7083 - accuracy: 0.548 - ETA: 20s - loss: 0.7094 - accuracy: 0.546 - ETA: 19s - loss: 0.7086 - accuracy: 0.547 - ETA: 17s - loss: 0.7090 - accuracy: 0.546 - ETA: 16s - loss: 0.7085 - accuracy: 0.548 - ETA: 14s - loss: 0.7082 - accuracy: 0.548 - ETA: 13s - loss: 0.7077 - accuracy: 0.549 - ETA: 11s - loss: 0.7080 - accuracy: 0.549 - ETA: 10s - loss: 0.7085 - accuracy: 0.547 - ETA: 8s - loss: 0.7085 - accuracy: 0.548 - ETA: 7s - loss: 0.7088 - accuracy: 0.54 - ETA: 5s - loss: 0.7079 - accuracy: 0.54 - ETA: 4s - loss: 0.7074 - accuracy: 0.54 - ETA: 2s - loss: 0.7071 - accuracy: 0.54 - ETA: 1s - loss: 0.7072 - accuracy: 0.54 - ETA: 0s - loss: 0.7069 - accuracy: 0.54 - 192s 2s/step - loss: 0.7069 - accuracy: 0.5490 - val_loss: 0.6834 - val_accuracy: 0.5907\n",
      "Epoch 2/2\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.6295 - accuracy: 0.65 - ETA: 1:24 - loss: 0.7052 - accuracy: 0.51 - ETA: 1:51 - loss: 0.6961 - accuracy: 0.54 - ETA: 2:03 - loss: 0.7023 - accuracy: 0.52 - ETA: 2:11 - loss: 0.7184 - accuracy: 0.50 - ETA: 2:15 - loss: 0.7269 - accuracy: 0.50 - ETA: 2:18 - loss: 0.7145 - accuracy: 0.50 - ETA: 2:19 - loss: 0.7045 - accuracy: 0.53 - ETA: 2:20 - loss: 0.7037 - accuracy: 0.53 - ETA: 2:20 - loss: 0.6968 - accuracy: 0.54 - ETA: 2:20 - loss: 0.7023 - accuracy: 0.53 - ETA: 2:20 - loss: 0.7079 - accuracy: 0.52 - ETA: 2:20 - loss: 0.7156 - accuracy: 0.51 - ETA: 2:19 - loss: 0.7177 - accuracy: 0.51 - ETA: 2:18 - loss: 0.7148 - accuracy: 0.52 - ETA: 2:18 - loss: 0.7149 - accuracy: 0.53 - ETA: 2:17 - loss: 0.7119 - accuracy: 0.53 - ETA: 2:16 - loss: 0.7095 - accuracy: 0.54 - ETA: 2:15 - loss: 0.7086 - accuracy: 0.54 - ETA: 2:14 - loss: 0.7105 - accuracy: 0.53 - ETA: 2:13 - loss: 0.7112 - accuracy: 0.53 - ETA: 2:11 - loss: 0.7076 - accuracy: 0.53 - ETA: 2:10 - loss: 0.7045 - accuracy: 0.54 - ETA: 2:09 - loss: 0.7046 - accuracy: 0.54 - ETA: 2:08 - loss: 0.7041 - accuracy: 0.54 - ETA: 2:07 - loss: 0.7030 - accuracy: 0.54 - ETA: 2:05 - loss: 0.7058 - accuracy: 0.53 - ETA: 2:04 - loss: 0.7051 - accuracy: 0.53 - ETA: 2:03 - loss: 0.7014 - accuracy: 0.54 - ETA: 2:02 - loss: 0.7029 - accuracy: 0.54 - ETA: 2:00 - loss: 0.7029 - accuracy: 0.53 - ETA: 1:59 - loss: 0.7023 - accuracy: 0.53 - ETA: 1:58 - loss: 0.7030 - accuracy: 0.53 - ETA: 1:56 - loss: 0.7037 - accuracy: 0.53 - ETA: 1:55 - loss: 0.7061 - accuracy: 0.53 - ETA: 1:54 - loss: 0.7065 - accuracy: 0.53 - ETA: 1:52 - loss: 0.7072 - accuracy: 0.53 - ETA: 1:51 - loss: 0.7072 - accuracy: 0.53 - ETA: 1:50 - loss: 0.7093 - accuracy: 0.53 - ETA: 1:48 - loss: 0.7085 - accuracy: 0.53 - ETA: 1:47 - loss: 0.7084 - accuracy: 0.53 - ETA: 1:45 - loss: 0.7083 - accuracy: 0.53 - ETA: 1:44 - loss: 0.7073 - accuracy: 0.54 - ETA: 1:43 - loss: 0.7055 - accuracy: 0.54 - ETA: 1:41 - loss: 0.7076 - accuracy: 0.53 - ETA: 1:40 - loss: 0.7061 - accuracy: 0.54 - ETA: 1:39 - loss: 0.7048 - accuracy: 0.54 - ETA: 1:37 - loss: 0.7050 - accuracy: 0.53 - ETA: 1:36 - loss: 0.7051 - accuracy: 0.53 - ETA: 1:34 - loss: 0.7056 - accuracy: 0.53 - ETA: 1:33 - loss: 0.7053 - accuracy: 0.53 - ETA: 1:31 - loss: 0.7067 - accuracy: 0.53 - ETA: 1:30 - loss: 0.7065 - accuracy: 0.53 - ETA: 1:29 - loss: 0.7054 - accuracy: 0.53 - ETA: 1:27 - loss: 0.7058 - accuracy: 0.53 - ETA: 1:26 - loss: 0.7055 - accuracy: 0.53 - ETA: 1:24 - loss: 0.7058 - accuracy: 0.53 - ETA: 1:23 - loss: 0.7058 - accuracy: 0.54 - ETA: 1:21 - loss: 0.7050 - accuracy: 0.54 - ETA: 1:20 - loss: 0.7038 - accuracy: 0.54 - ETA: 1:18 - loss: 0.7047 - accuracy: 0.54 - ETA: 1:17 - loss: 0.7048 - accuracy: 0.54 - ETA: 1:16 - loss: 0.7052 - accuracy: 0.54 - ETA: 1:14 - loss: 0.7041 - accuracy: 0.54 - ETA: 1:13 - loss: 0.7038 - accuracy: 0.54 - ETA: 1:11 - loss: 0.7041 - accuracy: 0.54 - ETA: 1:10 - loss: 0.7038 - accuracy: 0.54 - ETA: 1:08 - loss: 0.7046 - accuracy: 0.53 - ETA: 1:07 - loss: 0.7047 - accuracy: 0.53 - ETA: 1:05 - loss: 0.7051 - accuracy: 0.53 - ETA: 1:04 - loss: 0.7046 - accuracy: 0.53 - ETA: 1:03 - loss: 0.7053 - accuracy: 0.53 - ETA: 1:01 - loss: 0.7061 - accuracy: 0.53 - ETA: 1:00 - loss: 0.7056 - accuracy: 0.53 - ETA: 58s - loss: 0.7051 - accuracy: 0.5408 - ETA: 57s - loss: 0.7056 - accuracy: 0.539 - ETA: 55s - loss: 0.7051 - accuracy: 0.540 - ETA: 54s - loss: 0.7046 - accuracy: 0.541 - ETA: 52s - loss: 0.7040 - accuracy: 0.541 - ETA: 51s - loss: 0.7050 - accuracy: 0.542 - ETA: 49s - loss: 0.7041 - accuracy: 0.543 - ETA: 48s - loss: 0.7030 - accuracy: 0.545 - ETA: 47s - loss: 0.7022 - accuracy: 0.547 - ETA: 45s - loss: 0.7027 - accuracy: 0.547 - ETA: 44s - loss: 0.7031 - accuracy: 0.547 - ETA: 42s - loss: 0.7024 - accuracy: 0.547 - ETA: 41s - loss: 0.7019 - accuracy: 0.549 - ETA: 39s - loss: 0.7026 - accuracy: 0.547 - ETA: 38s - loss: 0.7028 - accuracy: 0.547 - ETA: 36s - loss: 0.7029 - accuracy: 0.547 - ETA: 35s - loss: 0.7023 - accuracy: 0.550 - ETA: 33s - loss: 0.7035 - accuracy: 0.548 - ETA: 32s - loss: 0.7026 - accuracy: 0.549 - ETA: 30s - loss: 0.7023 - accuracy: 0.549 - ETA: 29s - loss: 0.7018 - accuracy: 0.551 - ETA: 27s - loss: 0.7014 - accuracy: 0.551 - ETA: 26s - loss: 0.7026 - accuracy: 0.550 - ETA: 25s - loss: 0.7030 - accuracy: 0.549 - ETA: 23s - loss: 0.7028 - accuracy: 0.548 - ETA: 22s - loss: 0.7025 - accuracy: 0.549 - ETA: 20s - loss: 0.7039 - accuracy: 0.547 - ETA: 19s - loss: 0.7041 - accuracy: 0.547 - ETA: 17s - loss: 0.7043 - accuracy: 0.547 - ETA: 16s - loss: 0.7044 - accuracy: 0.548 - ETA: 14s - loss: 0.7043 - accuracy: 0.548 - ETA: 13s - loss: 0.7039 - accuracy: 0.548 - ETA: 11s - loss: 0.7044 - accuracy: 0.547 - ETA: 10s - loss: 0.7047 - accuracy: 0.546 - ETA: 8s - loss: 0.7049 - accuracy: 0.546 - ETA: 7s - loss: 0.7041 - accuracy: 0.54 - ETA: 5s - loss: 0.7049 - accuracy: 0.54 - ETA: 4s - loss: 0.7052 - accuracy: 0.54 - ETA: 2s - loss: 0.7054 - accuracy: 0.54 - ETA: 1s - loss: 0.7055 - accuracy: 0.54 - ETA: 0s - loss: 0.7051 - accuracy: 0.54 - 186s 2s/step - loss: 0.7051 - accuracy: 0.5446 - val_loss: 0.6831 - val_accuracy: 0.5907\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: bb2236e233825d444d954cd3e11a6f25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.5906862616539001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dense_units: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dropout_1: 0.30000000000000004</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dropout_2: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-learning_rate: 5e-05</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-lstm2_units: 128</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-optimizer_name: sgd</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/bracket: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/epochs: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/initial_epoch: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/round: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_idx (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segments (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 512, 768), ( 109482240   input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 input_segments[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           49216       global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            65          dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 109,531,521\n",
      "Trainable params: 49,281\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.7021 - accuracy: 0.50 - ETA: 1:22 - loss: 0.7000 - accuracy: 0.46 - ETA: 1:48 - loss: 0.7003 - accuracy: 0.46 - ETA: 2:01 - loss: 0.7040 - accuracy: 0.46 - ETA: 2:08 - loss: 0.7049 - accuracy: 0.46 - ETA: 2:12 - loss: 0.7074 - accuracy: 0.45 - ETA: 2:15 - loss: 0.7078 - accuracy: 0.46 - ETA: 2:16 - loss: 0.7092 - accuracy: 0.46 - ETA: 2:17 - loss: 0.7069 - accuracy: 0.47 - ETA: 2:18 - loss: 0.7015 - accuracy: 0.49 - ETA: 2:18 - loss: 0.7009 - accuracy: 0.49 - ETA: 2:18 - loss: 0.6998 - accuracy: 0.50 - ETA: 2:18 - loss: 0.7024 - accuracy: 0.50 - ETA: 2:17 - loss: 0.6992 - accuracy: 0.51 - ETA: 2:16 - loss: 0.6968 - accuracy: 0.51 - ETA: 2:16 - loss: 0.6988 - accuracy: 0.50 - ETA: 2:15 - loss: 0.6998 - accuracy: 0.50 - ETA: 2:14 - loss: 0.7021 - accuracy: 0.50 - ETA: 2:13 - loss: 0.7027 - accuracy: 0.50 - ETA: 2:12 - loss: 0.7031 - accuracy: 0.50 - ETA: 2:11 - loss: 0.7013 - accuracy: 0.50 - ETA: 2:10 - loss: 0.6988 - accuracy: 0.51 - ETA: 2:09 - loss: 0.6993 - accuracy: 0.51 - ETA: 2:08 - loss: 0.6994 - accuracy: 0.51 - ETA: 2:07 - loss: 0.6982 - accuracy: 0.52 - ETA: 2:06 - loss: 0.6973 - accuracy: 0.52 - ETA: 2:04 - loss: 0.6966 - accuracy: 0.52 - ETA: 2:03 - loss: 0.6973 - accuracy: 0.52 - ETA: 2:02 - loss: 0.6962 - accuracy: 0.52 - ETA: 2:01 - loss: 0.6959 - accuracy: 0.52 - ETA: 1:59 - loss: 0.6945 - accuracy: 0.52 - ETA: 1:58 - loss: 0.6925 - accuracy: 0.52 - ETA: 1:57 - loss: 0.6921 - accuracy: 0.53 - ETA: 1:55 - loss: 0.6923 - accuracy: 0.53 - ETA: 1:54 - loss: 0.6926 - accuracy: 0.53 - ETA: 1:53 - loss: 0.6927 - accuracy: 0.53 - ETA: 1:52 - loss: 0.6919 - accuracy: 0.53 - ETA: 1:50 - loss: 0.6925 - accuracy: 0.53 - ETA: 1:49 - loss: 0.6920 - accuracy: 0.53 - ETA: 1:47 - loss: 0.6924 - accuracy: 0.53 - ETA: 1:46 - loss: 0.6927 - accuracy: 0.53 - ETA: 1:45 - loss: 0.6937 - accuracy: 0.53 - ETA: 1:43 - loss: 0.6949 - accuracy: 0.52 - ETA: 1:42 - loss: 0.6959 - accuracy: 0.52 - ETA: 1:41 - loss: 0.6957 - accuracy: 0.52 - ETA: 1:39 - loss: 0.6970 - accuracy: 0.52 - ETA: 1:38 - loss: 0.6977 - accuracy: 0.52 - ETA: 1:36 - loss: 0.6968 - accuracy: 0.52 - ETA: 1:35 - loss: 0.6961 - accuracy: 0.52 - ETA: 1:34 - loss: 0.6952 - accuracy: 0.52 - ETA: 1:32 - loss: 0.6943 - accuracy: 0.53 - ETA: 1:31 - loss: 0.6943 - accuracy: 0.53 - ETA: 1:29 - loss: 0.6940 - accuracy: 0.53 - ETA: 1:28 - loss: 0.6949 - accuracy: 0.53 - ETA: 1:27 - loss: 0.6947 - accuracy: 0.53 - ETA: 1:25 - loss: 0.6953 - accuracy: 0.53 - ETA: 1:24 - loss: 0.6944 - accuracy: 0.53 - ETA: 1:22 - loss: 0.6946 - accuracy: 0.53 - ETA: 1:21 - loss: 0.6942 - accuracy: 0.53 - ETA: 1:20 - loss: 0.6940 - accuracy: 0.53 - ETA: 1:18 - loss: 0.6941 - accuracy: 0.53 - ETA: 1:17 - loss: 0.6940 - accuracy: 0.53 - ETA: 1:15 - loss: 0.6936 - accuracy: 0.53 - ETA: 1:14 - loss: 0.6927 - accuracy: 0.53 - ETA: 1:12 - loss: 0.6922 - accuracy: 0.54 - ETA: 1:11 - loss: 0.6924 - accuracy: 0.53 - ETA: 1:10 - loss: 0.6927 - accuracy: 0.53 - ETA: 1:08 - loss: 0.6936 - accuracy: 0.53 - ETA: 1:07 - loss: 0.6936 - accuracy: 0.53 - ETA: 1:05 - loss: 0.6936 - accuracy: 0.53 - ETA: 1:04 - loss: 0.6936 - accuracy: 0.53 - ETA: 1:02 - loss: 0.6935 - accuracy: 0.53 - ETA: 1:01 - loss: 0.6925 - accuracy: 0.54 - ETA: 59s - loss: 0.6930 - accuracy: 0.5393 - ETA: 58s - loss: 0.6931 - accuracy: 0.538 - ETA: 57s - loss: 0.6930 - accuracy: 0.536 - ETA: 55s - loss: 0.6936 - accuracy: 0.535 - ETA: 54s - loss: 0.6934 - accuracy: 0.536 - ETA: 52s - loss: 0.6944 - accuracy: 0.534 - ETA: 51s - loss: 0.6944 - accuracy: 0.534 - ETA: 49s - loss: 0.6942 - accuracy: 0.533 - ETA: 48s - loss: 0.6942 - accuracy: 0.534 - ETA: 46s - loss: 0.6943 - accuracy: 0.533 - ETA: 45s - loss: 0.6940 - accuracy: 0.533 - ETA: 43s - loss: 0.6943 - accuracy: 0.532 - ETA: 42s - loss: 0.6947 - accuracy: 0.532 - ETA: 41s - loss: 0.6947 - accuracy: 0.533 - ETA: 39s - loss: 0.6947 - accuracy: 0.532 - ETA: 38s - loss: 0.6947 - accuracy: 0.533 - ETA: 36s - loss: 0.6947 - accuracy: 0.534 - ETA: 35s - loss: 0.6942 - accuracy: 0.535 - ETA: 33s - loss: 0.6941 - accuracy: 0.535 - ETA: 32s - loss: 0.6941 - accuracy: 0.535 - ETA: 30s - loss: 0.6940 - accuracy: 0.535 - ETA: 29s - loss: 0.6939 - accuracy: 0.535 - ETA: 27s - loss: 0.6940 - accuracy: 0.533 - ETA: 26s - loss: 0.6941 - accuracy: 0.533 - ETA: 24s - loss: 0.6939 - accuracy: 0.535 - ETA: 23s - loss: 0.6939 - accuracy: 0.536 - ETA: 22s - loss: 0.6937 - accuracy: 0.536 - ETA: 20s - loss: 0.6934 - accuracy: 0.536 - ETA: 19s - loss: 0.6936 - accuracy: 0.536 - ETA: 17s - loss: 0.6941 - accuracy: 0.536 - ETA: 16s - loss: 0.6938 - accuracy: 0.537 - ETA: 14s - loss: 0.6941 - accuracy: 0.536 - ETA: 13s - loss: 0.6943 - accuracy: 0.537 - ETA: 11s - loss: 0.6942 - accuracy: 0.537 - ETA: 10s - loss: 0.6948 - accuracy: 0.536 - ETA: 8s - loss: 0.6947 - accuracy: 0.536 - ETA: 7s - loss: 0.6946 - accuracy: 0.53 - ETA: 5s - loss: 0.6947 - accuracy: 0.53 - ETA: 4s - loss: 0.6946 - accuracy: 0.53 - ETA: 2s - loss: 0.6950 - accuracy: 0.53 - ETA: 1s - loss: 0.6951 - accuracy: 0.53 - ETA: 0s - loss: 0.6951 - accuracy: 0.53 - 191s 2s/step - loss: 0.6951 - accuracy: 0.5365 - val_loss: 0.6852 - val_accuracy: 0.5809\n",
      "Epoch 2/2\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.6923 - accuracy: 0.53 - ETA: 1:23 - loss: 0.7090 - accuracy: 0.50 - ETA: 1:50 - loss: 0.7141 - accuracy: 0.48 - ETA: 2:03 - loss: 0.7175 - accuracy: 0.46 - ETA: 2:10 - loss: 0.7167 - accuracy: 0.45 - ETA: 2:14 - loss: 0.7138 - accuracy: 0.46 - ETA: 2:18 - loss: 0.7039 - accuracy: 0.48 - ETA: 2:19 - loss: 0.7041 - accuracy: 0.48 - ETA: 2:20 - loss: 0.7078 - accuracy: 0.47 - ETA: 2:20 - loss: 0.7113 - accuracy: 0.47 - ETA: 2:20 - loss: 0.7061 - accuracy: 0.48 - ETA: 2:20 - loss: 0.7052 - accuracy: 0.48 - ETA: 2:20 - loss: 0.7070 - accuracy: 0.47 - ETA: 2:19 - loss: 0.7075 - accuracy: 0.47 - ETA: 2:18 - loss: 0.7084 - accuracy: 0.47 - ETA: 2:18 - loss: 0.7035 - accuracy: 0.48 - ETA: 2:17 - loss: 0.6991 - accuracy: 0.49 - ETA: 2:16 - loss: 0.6995 - accuracy: 0.50 - ETA: 2:15 - loss: 0.6983 - accuracy: 0.50 - ETA: 2:14 - loss: 0.6984 - accuracy: 0.50 - ETA: 2:13 - loss: 0.6978 - accuracy: 0.51 - ETA: 2:12 - loss: 0.6971 - accuracy: 0.51 - ETA: 2:11 - loss: 0.6953 - accuracy: 0.51 - ETA: 2:09 - loss: 0.6962 - accuracy: 0.51 - ETA: 2:08 - loss: 0.6969 - accuracy: 0.51 - ETA: 2:07 - loss: 0.6985 - accuracy: 0.50 - ETA: 2:06 - loss: 0.6976 - accuracy: 0.51 - ETA: 2:04 - loss: 0.6963 - accuracy: 0.51 - ETA: 2:03 - loss: 0.6964 - accuracy: 0.51 - ETA: 2:02 - loss: 0.6979 - accuracy: 0.51 - ETA: 2:00 - loss: 0.6983 - accuracy: 0.51 - ETA: 1:59 - loss: 0.6976 - accuracy: 0.51 - ETA: 1:58 - loss: 0.6984 - accuracy: 0.51 - ETA: 1:56 - loss: 0.6981 - accuracy: 0.51 - ETA: 1:55 - loss: 0.6980 - accuracy: 0.51 - ETA: 1:54 - loss: 0.6969 - accuracy: 0.51 - ETA: 1:52 - loss: 0.6966 - accuracy: 0.51 - ETA: 1:51 - loss: 0.6965 - accuracy: 0.51 - ETA: 1:50 - loss: 0.6978 - accuracy: 0.51 - ETA: 1:48 - loss: 0.6982 - accuracy: 0.50 - ETA: 1:47 - loss: 0.6984 - accuracy: 0.50 - ETA: 1:45 - loss: 0.6983 - accuracy: 0.50 - ETA: 1:44 - loss: 0.6983 - accuracy: 0.50 - ETA: 1:43 - loss: 0.6985 - accuracy: 0.50 - ETA: 1:41 - loss: 0.6985 - accuracy: 0.50 - ETA: 1:40 - loss: 0.6996 - accuracy: 0.50 - ETA: 1:38 - loss: 0.7005 - accuracy: 0.50 - ETA: 1:37 - loss: 0.7010 - accuracy: 0.49 - ETA: 1:36 - loss: 0.7023 - accuracy: 0.49 - ETA: 1:34 - loss: 0.7019 - accuracy: 0.49 - ETA: 1:33 - loss: 0.7024 - accuracy: 0.49 - ETA: 1:31 - loss: 0.7024 - accuracy: 0.49 - ETA: 1:30 - loss: 0.7028 - accuracy: 0.49 - ETA: 1:29 - loss: 0.7027 - accuracy: 0.49 - ETA: 1:27 - loss: 0.7018 - accuracy: 0.49 - ETA: 1:26 - loss: 0.7011 - accuracy: 0.49 - ETA: 1:24 - loss: 0.7023 - accuracy: 0.49 - ETA: 1:23 - loss: 0.7022 - accuracy: 0.49 - ETA: 1:21 - loss: 0.7018 - accuracy: 0.49 - ETA: 1:20 - loss: 0.7019 - accuracy: 0.49 - ETA: 1:19 - loss: 0.7026 - accuracy: 0.49 - ETA: 1:17 - loss: 0.7038 - accuracy: 0.49 - ETA: 1:16 - loss: 0.7039 - accuracy: 0.49 - ETA: 1:14 - loss: 0.7049 - accuracy: 0.49 - ETA: 1:13 - loss: 0.7051 - accuracy: 0.49 - ETA: 1:11 - loss: 0.7045 - accuracy: 0.49 - ETA: 1:10 - loss: 0.7044 - accuracy: 0.49 - ETA: 1:08 - loss: 0.7051 - accuracy: 0.49 - ETA: 1:07 - loss: 0.7050 - accuracy: 0.49 - ETA: 1:05 - loss: 0.7046 - accuracy: 0.49 - ETA: 1:04 - loss: 0.7044 - accuracy: 0.49 - ETA: 1:03 - loss: 0.7048 - accuracy: 0.49 - ETA: 1:01 - loss: 0.7045 - accuracy: 0.49 - ETA: 1:00 - loss: 0.7048 - accuracy: 0.49 - ETA: 58s - loss: 0.7047 - accuracy: 0.4933 - ETA: 57s - loss: 0.7046 - accuracy: 0.494 - ETA: 55s - loss: 0.7038 - accuracy: 0.496 - ETA: 54s - loss: 0.7043 - accuracy: 0.495 - ETA: 52s - loss: 0.7042 - accuracy: 0.496 - ETA: 51s - loss: 0.7045 - accuracy: 0.495 - ETA: 49s - loss: 0.7048 - accuracy: 0.495 - ETA: 48s - loss: 0.7047 - accuracy: 0.495 - ETA: 46s - loss: 0.7045 - accuracy: 0.496 - ETA: 45s - loss: 0.7047 - accuracy: 0.495 - ETA: 44s - loss: 0.7048 - accuracy: 0.495 - ETA: 42s - loss: 0.7044 - accuracy: 0.496 - ETA: 41s - loss: 0.7037 - accuracy: 0.498 - ETA: 39s - loss: 0.7036 - accuracy: 0.498 - ETA: 38s - loss: 0.7039 - accuracy: 0.498 - ETA: 36s - loss: 0.7043 - accuracy: 0.498 - ETA: 35s - loss: 0.7040 - accuracy: 0.499 - ETA: 33s - loss: 0.7042 - accuracy: 0.499 - ETA: 32s - loss: 0.7039 - accuracy: 0.499 - ETA: 30s - loss: 0.7037 - accuracy: 0.499 - ETA: 29s - loss: 0.7033 - accuracy: 0.500 - ETA: 27s - loss: 0.7035 - accuracy: 0.500 - ETA: 26s - loss: 0.7035 - accuracy: 0.500 - ETA: 25s - loss: 0.7034 - accuracy: 0.499 - ETA: 23s - loss: 0.7035 - accuracy: 0.498 - ETA: 22s - loss: 0.7031 - accuracy: 0.499 - ETA: 20s - loss: 0.7029 - accuracy: 0.500 - ETA: 19s - loss: 0.7030 - accuracy: 0.499 - ETA: 17s - loss: 0.7027 - accuracy: 0.500 - ETA: 16s - loss: 0.7026 - accuracy: 0.501 - ETA: 14s - loss: 0.7027 - accuracy: 0.501 - ETA: 13s - loss: 0.7027 - accuracy: 0.502 - ETA: 11s - loss: 0.7028 - accuracy: 0.503 - ETA: 10s - loss: 0.7026 - accuracy: 0.503 - ETA: 8s - loss: 0.7024 - accuracy: 0.504 - ETA: 7s - loss: 0.7019 - accuracy: 0.50 - ETA: 5s - loss: 0.7017 - accuracy: 0.50 - ETA: 4s - loss: 0.7017 - accuracy: 0.50 - ETA: 2s - loss: 0.7019 - accuracy: 0.50 - ETA: 1s - loss: 0.7021 - accuracy: 0.50 - ETA: 0s - loss: 0.7020 - accuracy: 0.50 - 185s 2s/step - loss: 0.7020 - accuracy: 0.5060 - val_loss: 0.6850 - val_accuracy: 0.5760\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 41bdb8a483967fe20bc4414ab0c15e76</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.5808823704719543</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dense_units: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dropout_1: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dropout_2: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-learning_rate: 2e-05</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-lstm2_units: 512</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-optimizer_name: sgd</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/bracket: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/epochs: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/initial_epoch: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/round: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_idx (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segments (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 512, 768), ( 109482240   input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 input_segments[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           49216       global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            65          dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 109,531,521\n",
      "Trainable params: 49,281\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.6680 - accuracy: 0.59 - ETA: 1:22 - loss: 0.6873 - accuracy: 0.56 - ETA: 1:48 - loss: 0.6857 - accuracy: 0.57 - ETA: 2:01 - loss: 0.6836 - accuracy: 0.58 - ETA: 2:08 - loss: 0.6842 - accuracy: 0.60 - ETA: 2:12 - loss: 0.6928 - accuracy: 0.57 - ETA: 2:15 - loss: 0.7005 - accuracy: 0.54 - ETA: 2:16 - loss: 0.6991 - accuracy: 0.54 - ETA: 2:17 - loss: 0.6998 - accuracy: 0.53 - ETA: 2:18 - loss: 0.7001 - accuracy: 0.53 - ETA: 2:18 - loss: 0.7041 - accuracy: 0.51 - ETA: 2:18 - loss: 0.7024 - accuracy: 0.52 - ETA: 2:17 - loss: 0.7013 - accuracy: 0.53 - ETA: 2:17 - loss: 0.7006 - accuracy: 0.53 - ETA: 2:16 - loss: 0.7007 - accuracy: 0.53 - ETA: 2:15 - loss: 0.7017 - accuracy: 0.52 - ETA: 2:15 - loss: 0.7032 - accuracy: 0.51 - ETA: 2:14 - loss: 0.7033 - accuracy: 0.51 - ETA: 2:13 - loss: 0.7028 - accuracy: 0.52 - ETA: 2:12 - loss: 0.7036 - accuracy: 0.52 - ETA: 2:11 - loss: 0.7038 - accuracy: 0.52 - ETA: 2:10 - loss: 0.7020 - accuracy: 0.52 - ETA: 2:09 - loss: 0.7034 - accuracy: 0.52 - ETA: 2:08 - loss: 0.7036 - accuracy: 0.52 - ETA: 2:07 - loss: 0.7021 - accuracy: 0.53 - ETA: 2:05 - loss: 0.7028 - accuracy: 0.52 - ETA: 2:04 - loss: 0.7019 - accuracy: 0.52 - ETA: 2:03 - loss: 0.7025 - accuracy: 0.52 - ETA: 2:02 - loss: 0.7024 - accuracy: 0.52 - ETA: 2:00 - loss: 0.7013 - accuracy: 0.52 - ETA: 1:59 - loss: 0.7024 - accuracy: 0.52 - ETA: 1:58 - loss: 0.7018 - accuracy: 0.52 - ETA: 1:57 - loss: 0.7014 - accuracy: 0.52 - ETA: 1:55 - loss: 0.7012 - accuracy: 0.52 - ETA: 1:54 - loss: 0.7011 - accuracy: 0.52 - ETA: 1:53 - loss: 0.7003 - accuracy: 0.52 - ETA: 1:51 - loss: 0.7007 - accuracy: 0.52 - ETA: 1:50 - loss: 0.7004 - accuracy: 0.52 - ETA: 1:49 - loss: 0.7003 - accuracy: 0.52 - ETA: 1:47 - loss: 0.7001 - accuracy: 0.52 - ETA: 1:46 - loss: 0.6998 - accuracy: 0.52 - ETA: 1:45 - loss: 0.6999 - accuracy: 0.52 - ETA: 1:43 - loss: 0.6986 - accuracy: 0.53 - ETA: 1:42 - loss: 0.6999 - accuracy: 0.52 - ETA: 1:41 - loss: 0.7001 - accuracy: 0.52 - ETA: 1:39 - loss: 0.7011 - accuracy: 0.52 - ETA: 1:38 - loss: 0.7008 - accuracy: 0.52 - ETA: 1:36 - loss: 0.7009 - accuracy: 0.52 - ETA: 1:35 - loss: 0.7006 - accuracy: 0.52 - ETA: 1:34 - loss: 0.7005 - accuracy: 0.52 - ETA: 1:32 - loss: 0.7009 - accuracy: 0.52 - ETA: 1:31 - loss: 0.7005 - accuracy: 0.52 - ETA: 1:29 - loss: 0.7010 - accuracy: 0.52 - ETA: 1:28 - loss: 0.7017 - accuracy: 0.52 - ETA: 1:27 - loss: 0.7018 - accuracy: 0.52 - ETA: 1:25 - loss: 0.7015 - accuracy: 0.52 - ETA: 1:24 - loss: 0.7014 - accuracy: 0.52 - ETA: 1:22 - loss: 0.7010 - accuracy: 0.52 - ETA: 1:21 - loss: 0.7008 - accuracy: 0.52 - ETA: 1:19 - loss: 0.7006 - accuracy: 0.52 - ETA: 1:18 - loss: 0.7006 - accuracy: 0.52 - ETA: 1:17 - loss: 0.7010 - accuracy: 0.52 - ETA: 1:15 - loss: 0.7009 - accuracy: 0.52 - ETA: 1:14 - loss: 0.6999 - accuracy: 0.52 - ETA: 1:12 - loss: 0.7000 - accuracy: 0.52 - ETA: 1:11 - loss: 0.6996 - accuracy: 0.53 - ETA: 1:09 - loss: 0.6997 - accuracy: 0.53 - ETA: 1:08 - loss: 0.6998 - accuracy: 0.53 - ETA: 1:07 - loss: 0.6997 - accuracy: 0.53 - ETA: 1:05 - loss: 0.6997 - accuracy: 0.53 - ETA: 1:04 - loss: 0.6994 - accuracy: 0.53 - ETA: 1:02 - loss: 0.6987 - accuracy: 0.53 - ETA: 1:01 - loss: 0.6988 - accuracy: 0.53 - ETA: 59s - loss: 0.6995 - accuracy: 0.5321 - ETA: 58s - loss: 0.6997 - accuracy: 0.531 - ETA: 56s - loss: 0.6998 - accuracy: 0.531 - ETA: 55s - loss: 0.7007 - accuracy: 0.529 - ETA: 54s - loss: 0.7005 - accuracy: 0.529 - ETA: 52s - loss: 0.7003 - accuracy: 0.530 - ETA: 51s - loss: 0.7004 - accuracy: 0.529 - ETA: 49s - loss: 0.7009 - accuracy: 0.527 - ETA: 48s - loss: 0.7006 - accuracy: 0.528 - ETA: 46s - loss: 0.7009 - accuracy: 0.527 - ETA: 45s - loss: 0.7002 - accuracy: 0.530 - ETA: 43s - loss: 0.7000 - accuracy: 0.530 - ETA: 42s - loss: 0.7002 - accuracy: 0.529 - ETA: 41s - loss: 0.7000 - accuracy: 0.529 - ETA: 39s - loss: 0.6997 - accuracy: 0.530 - ETA: 38s - loss: 0.6994 - accuracy: 0.530 - ETA: 36s - loss: 0.6991 - accuracy: 0.531 - ETA: 35s - loss: 0.6989 - accuracy: 0.533 - ETA: 33s - loss: 0.6990 - accuracy: 0.532 - ETA: 32s - loss: 0.6988 - accuracy: 0.533 - ETA: 30s - loss: 0.6984 - accuracy: 0.534 - ETA: 29s - loss: 0.6984 - accuracy: 0.534 - ETA: 27s - loss: 0.6982 - accuracy: 0.534 - ETA: 26s - loss: 0.6978 - accuracy: 0.534 - ETA: 24s - loss: 0.6978 - accuracy: 0.535 - ETA: 23s - loss: 0.6977 - accuracy: 0.535 - ETA: 22s - loss: 0.6978 - accuracy: 0.534 - ETA: 20s - loss: 0.6981 - accuracy: 0.533 - ETA: 19s - loss: 0.6982 - accuracy: 0.532 - ETA: 17s - loss: 0.6981 - accuracy: 0.533 - ETA: 16s - loss: 0.6975 - accuracy: 0.534 - ETA: 14s - loss: 0.6972 - accuracy: 0.535 - ETA: 13s - loss: 0.6970 - accuracy: 0.536 - ETA: 11s - loss: 0.6972 - accuracy: 0.535 - ETA: 10s - loss: 0.6975 - accuracy: 0.535 - ETA: 8s - loss: 0.6973 - accuracy: 0.535 - ETA: 7s - loss: 0.6973 - accuracy: 0.53 - ETA: 5s - loss: 0.6970 - accuracy: 0.53 - ETA: 4s - loss: 0.6968 - accuracy: 0.53 - ETA: 2s - loss: 0.6968 - accuracy: 0.53 - ETA: 1s - loss: 0.6968 - accuracy: 0.53 - ETA: 0s - loss: 0.6965 - accuracy: 0.53 - 191s 2s/step - loss: 0.6965 - accuracy: 0.5386 - val_loss: 0.6902 - val_accuracy: 0.5907\n",
      "Epoch 2/2\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.6511 - accuracy: 0.65 - ETA: 1:22 - loss: 0.6883 - accuracy: 0.57 - ETA: 1:49 - loss: 0.6897 - accuracy: 0.57 - ETA: 2:02 - loss: 0.6917 - accuracy: 0.56 - ETA: 2:09 - loss: 0.6977 - accuracy: 0.53 - ETA: 2:14 - loss: 0.6936 - accuracy: 0.53 - ETA: 2:16 - loss: 0.6867 - accuracy: 0.55 - ETA: 2:18 - loss: 0.6866 - accuracy: 0.55 - ETA: 2:19 - loss: 0.6897 - accuracy: 0.54 - ETA: 2:19 - loss: 0.6874 - accuracy: 0.55 - ETA: 2:19 - loss: 0.6887 - accuracy: 0.54 - ETA: 2:19 - loss: 0.6914 - accuracy: 0.53 - ETA: 2:19 - loss: 0.6935 - accuracy: 0.52 - ETA: 2:18 - loss: 0.6922 - accuracy: 0.52 - ETA: 2:18 - loss: 0.6928 - accuracy: 0.52 - ETA: 2:17 - loss: 0.6903 - accuracy: 0.53 - ETA: 2:17 - loss: 0.6877 - accuracy: 0.54 - ETA: 2:16 - loss: 0.6877 - accuracy: 0.54 - ETA: 2:15 - loss: 0.6879 - accuracy: 0.54 - ETA: 2:14 - loss: 0.6906 - accuracy: 0.54 - ETA: 2:13 - loss: 0.6897 - accuracy: 0.54 - ETA: 2:12 - loss: 0.6874 - accuracy: 0.54 - ETA: 2:10 - loss: 0.6864 - accuracy: 0.55 - ETA: 2:09 - loss: 0.6869 - accuracy: 0.55 - ETA: 2:08 - loss: 0.6877 - accuracy: 0.55 - ETA: 2:07 - loss: 0.6877 - accuracy: 0.55 - ETA: 2:05 - loss: 0.6885 - accuracy: 0.54 - ETA: 2:04 - loss: 0.6886 - accuracy: 0.55 - ETA: 2:03 - loss: 0.6869 - accuracy: 0.55 - ETA: 2:02 - loss: 0.6884 - accuracy: 0.55 - ETA: 2:00 - loss: 0.6887 - accuracy: 0.54 - ETA: 1:59 - loss: 0.6875 - accuracy: 0.55 - ETA: 1:58 - loss: 0.6868 - accuracy: 0.55 - ETA: 1:56 - loss: 0.6862 - accuracy: 0.55 - ETA: 1:55 - loss: 0.6869 - accuracy: 0.55 - ETA: 1:54 - loss: 0.6874 - accuracy: 0.55 - ETA: 1:52 - loss: 0.6875 - accuracy: 0.55 - ETA: 1:51 - loss: 0.6876 - accuracy: 0.55 - ETA: 1:50 - loss: 0.6879 - accuracy: 0.55 - ETA: 1:48 - loss: 0.6878 - accuracy: 0.55 - ETA: 1:47 - loss: 0.6878 - accuracy: 0.55 - ETA: 1:45 - loss: 0.6873 - accuracy: 0.55 - ETA: 1:44 - loss: 0.6870 - accuracy: 0.55 - ETA: 1:43 - loss: 0.6867 - accuracy: 0.55 - ETA: 1:41 - loss: 0.6874 - accuracy: 0.55 - ETA: 1:40 - loss: 0.6875 - accuracy: 0.55 - ETA: 1:38 - loss: 0.6877 - accuracy: 0.55 - ETA: 1:37 - loss: 0.6880 - accuracy: 0.55 - ETA: 1:36 - loss: 0.6882 - accuracy: 0.55 - ETA: 1:34 - loss: 0.6881 - accuracy: 0.55 - ETA: 1:33 - loss: 0.6880 - accuracy: 0.55 - ETA: 1:31 - loss: 0.6881 - accuracy: 0.55 - ETA: 1:30 - loss: 0.6879 - accuracy: 0.55 - ETA: 1:28 - loss: 0.6882 - accuracy: 0.55 - ETA: 1:27 - loss: 0.6879 - accuracy: 0.55 - ETA: 1:26 - loss: 0.6881 - accuracy: 0.55 - ETA: 1:24 - loss: 0.6879 - accuracy: 0.55 - ETA: 1:23 - loss: 0.6878 - accuracy: 0.55 - ETA: 1:21 - loss: 0.6878 - accuracy: 0.55 - ETA: 1:20 - loss: 0.6877 - accuracy: 0.55 - ETA: 1:18 - loss: 0.6881 - accuracy: 0.55 - ETA: 1:17 - loss: 0.6879 - accuracy: 0.55 - ETA: 1:16 - loss: 0.6877 - accuracy: 0.55 - ETA: 1:14 - loss: 0.6876 - accuracy: 0.55 - ETA: 1:13 - loss: 0.6874 - accuracy: 0.55 - ETA: 1:11 - loss: 0.6872 - accuracy: 0.55 - ETA: 1:10 - loss: 0.6878 - accuracy: 0.55 - ETA: 1:08 - loss: 0.6881 - accuracy: 0.55 - ETA: 1:07 - loss: 0.6878 - accuracy: 0.55 - ETA: 1:05 - loss: 0.6874 - accuracy: 0.55 - ETA: 1:04 - loss: 0.6871 - accuracy: 0.55 - ETA: 1:03 - loss: 0.6873 - accuracy: 0.55 - ETA: 1:01 - loss: 0.6869 - accuracy: 0.55 - ETA: 1:00 - loss: 0.6871 - accuracy: 0.55 - ETA: 58s - loss: 0.6870 - accuracy: 0.5546 - ETA: 57s - loss: 0.6874 - accuracy: 0.553 - ETA: 55s - loss: 0.6870 - accuracy: 0.554 - ETA: 54s - loss: 0.6867 - accuracy: 0.556 - ETA: 52s - loss: 0.6862 - accuracy: 0.558 - ETA: 51s - loss: 0.6861 - accuracy: 0.559 - ETA: 49s - loss: 0.6859 - accuracy: 0.560 - ETA: 48s - loss: 0.6858 - accuracy: 0.559 - ETA: 46s - loss: 0.6858 - accuracy: 0.560 - ETA: 45s - loss: 0.6862 - accuracy: 0.559 - ETA: 44s - loss: 0.6865 - accuracy: 0.558 - ETA: 42s - loss: 0.6858 - accuracy: 0.561 - ETA: 41s - loss: 0.6855 - accuracy: 0.562 - ETA: 39s - loss: 0.6858 - accuracy: 0.561 - ETA: 38s - loss: 0.6862 - accuracy: 0.560 - ETA: 36s - loss: 0.6861 - accuracy: 0.560 - ETA: 35s - loss: 0.6859 - accuracy: 0.561 - ETA: 33s - loss: 0.6863 - accuracy: 0.560 - ETA: 32s - loss: 0.6860 - accuracy: 0.561 - ETA: 30s - loss: 0.6856 - accuracy: 0.562 - ETA: 29s - loss: 0.6853 - accuracy: 0.563 - ETA: 27s - loss: 0.6850 - accuracy: 0.563 - ETA: 26s - loss: 0.6852 - accuracy: 0.562 - ETA: 25s - loss: 0.6856 - accuracy: 0.561 - ETA: 23s - loss: 0.6854 - accuracy: 0.562 - ETA: 22s - loss: 0.6851 - accuracy: 0.563 - ETA: 20s - loss: 0.6856 - accuracy: 0.562 - ETA: 19s - loss: 0.6851 - accuracy: 0.563 - ETA: 17s - loss: 0.6847 - accuracy: 0.564 - ETA: 16s - loss: 0.6849 - accuracy: 0.564 - ETA: 14s - loss: 0.6847 - accuracy: 0.564 - ETA: 13s - loss: 0.6845 - accuracy: 0.565 - ETA: 11s - loss: 0.6849 - accuracy: 0.564 - ETA: 10s - loss: 0.6849 - accuracy: 0.564 - ETA: 8s - loss: 0.6848 - accuracy: 0.564 - ETA: 7s - loss: 0.6842 - accuracy: 0.56 - ETA: 5s - loss: 0.6842 - accuracy: 0.56 - ETA: 4s - loss: 0.6840 - accuracy: 0.56 - ETA: 2s - loss: 0.6837 - accuracy: 0.56 - ETA: 1s - loss: 0.6839 - accuracy: 0.56 - ETA: 0s - loss: 0.6839 - accuracy: 0.56 - 186s 2s/step - loss: 0.6839 - accuracy: 0.5659 - val_loss: 0.6790 - val_accuracy: 0.5907\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 9374edbf28410eacb5fd6eb0cfd08691</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.5906862616539001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dense_units: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dropout_1: 0.30000000000000004</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dropout_2: 0.4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-learning_rate: 3e-05</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-lstm2_units: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-optimizer_name: adam</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/bracket: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/epochs: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/initial_epoch: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/round: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_idx (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segments (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 512, 768), ( 109482240   input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 input_segments[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 109,679,361\n",
      "Trainable params: 197,121\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 3/4\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.6518 - accuracy: 0.59 - ETA: 1:23 - loss: 0.6960 - accuracy: 0.51 - ETA: 1:48 - loss: 0.6956 - accuracy: 0.51 - ETA: 2:01 - loss: 0.6884 - accuracy: 0.53 - ETA: 2:08 - loss: 0.6869 - accuracy: 0.54 - ETA: 2:12 - loss: 0.6910 - accuracy: 0.54 - ETA: 2:15 - loss: 0.6894 - accuracy: 0.53 - ETA: 2:16 - loss: 0.6896 - accuracy: 0.53 - ETA: 2:17 - loss: 0.6888 - accuracy: 0.52 - ETA: 2:18 - loss: 0.6859 - accuracy: 0.53 - ETA: 2:18 - loss: 0.6847 - accuracy: 0.53 - ETA: 2:18 - loss: 0.6842 - accuracy: 0.52 - ETA: 2:17 - loss: 0.6827 - accuracy: 0.54 - ETA: 2:17 - loss: 0.6831 - accuracy: 0.53 - ETA: 2:16 - loss: 0.6827 - accuracy: 0.53 - ETA: 2:15 - loss: 0.6839 - accuracy: 0.54 - ETA: 2:15 - loss: 0.6827 - accuracy: 0.54 - ETA: 2:14 - loss: 0.6831 - accuracy: 0.54 - ETA: 2:13 - loss: 0.6800 - accuracy: 0.55 - ETA: 2:12 - loss: 0.6817 - accuracy: 0.54 - ETA: 2:11 - loss: 0.6800 - accuracy: 0.55 - ETA: 2:10 - loss: 0.6780 - accuracy: 0.55 - ETA: 2:09 - loss: 0.6780 - accuracy: 0.55 - ETA: 2:07 - loss: 0.6793 - accuracy: 0.55 - ETA: 2:06 - loss: 0.6781 - accuracy: 0.56 - ETA: 2:05 - loss: 0.6791 - accuracy: 0.56 - ETA: 2:04 - loss: 0.6807 - accuracy: 0.55 - ETA: 2:03 - loss: 0.6816 - accuracy: 0.55 - ETA: 2:02 - loss: 0.6820 - accuracy: 0.55 - ETA: 2:00 - loss: 0.6803 - accuracy: 0.56 - ETA: 1:59 - loss: 0.6805 - accuracy: 0.56 - ETA: 1:58 - loss: 0.6796 - accuracy: 0.56 - ETA: 1:57 - loss: 0.6795 - accuracy: 0.56 - ETA: 1:55 - loss: 0.6790 - accuracy: 0.56 - ETA: 1:54 - loss: 0.6800 - accuracy: 0.56 - ETA: 1:53 - loss: 0.6792 - accuracy: 0.56 - ETA: 1:51 - loss: 0.6806 - accuracy: 0.56 - ETA: 1:50 - loss: 0.6812 - accuracy: 0.56 - ETA: 1:49 - loss: 0.6810 - accuracy: 0.56 - ETA: 1:47 - loss: 0.6797 - accuracy: 0.56 - ETA: 1:46 - loss: 0.6788 - accuracy: 0.57 - ETA: 1:45 - loss: 0.6785 - accuracy: 0.57 - ETA: 1:43 - loss: 0.6790 - accuracy: 0.57 - ETA: 1:42 - loss: 0.6812 - accuracy: 0.56 - ETA: 1:41 - loss: 0.6814 - accuracy: 0.56 - ETA: 1:39 - loss: 0.6816 - accuracy: 0.56 - ETA: 1:38 - loss: 0.6815 - accuracy: 0.56 - ETA: 1:37 - loss: 0.6811 - accuracy: 0.56 - ETA: 1:35 - loss: 0.6804 - accuracy: 0.56 - ETA: 1:34 - loss: 0.6799 - accuracy: 0.56 - ETA: 1:32 - loss: 0.6800 - accuracy: 0.56 - ETA: 1:31 - loss: 0.6802 - accuracy: 0.56 - ETA: 1:29 - loss: 0.6814 - accuracy: 0.56 - ETA: 1:28 - loss: 0.6808 - accuracy: 0.56 - ETA: 1:27 - loss: 0.6821 - accuracy: 0.56 - ETA: 1:25 - loss: 0.6828 - accuracy: 0.56 - ETA: 1:24 - loss: 0.6822 - accuracy: 0.56 - ETA: 1:22 - loss: 0.6814 - accuracy: 0.56 - ETA: 1:21 - loss: 0.6814 - accuracy: 0.56 - ETA: 1:20 - loss: 0.6813 - accuracy: 0.56 - ETA: 1:18 - loss: 0.6808 - accuracy: 0.56 - ETA: 1:17 - loss: 0.6806 - accuracy: 0.56 - ETA: 1:15 - loss: 0.6806 - accuracy: 0.56 - ETA: 1:14 - loss: 0.6796 - accuracy: 0.56 - ETA: 1:12 - loss: 0.6799 - accuracy: 0.56 - ETA: 1:11 - loss: 0.6793 - accuracy: 0.56 - ETA: 1:09 - loss: 0.6789 - accuracy: 0.56 - ETA: 1:08 - loss: 0.6794 - accuracy: 0.56 - ETA: 1:07 - loss: 0.6805 - accuracy: 0.56 - ETA: 1:05 - loss: 0.6800 - accuracy: 0.56 - ETA: 1:04 - loss: 0.6795 - accuracy: 0.56 - ETA: 1:02 - loss: 0.6795 - accuracy: 0.56 - ETA: 1:01 - loss: 0.6792 - accuracy: 0.56 - ETA: 59s - loss: 0.6798 - accuracy: 0.5655 - ETA: 58s - loss: 0.6794 - accuracy: 0.565 - ETA: 57s - loss: 0.6794 - accuracy: 0.565 - ETA: 55s - loss: 0.6797 - accuracy: 0.564 - ETA: 54s - loss: 0.6794 - accuracy: 0.566 - ETA: 52s - loss: 0.6791 - accuracy: 0.566 - ETA: 51s - loss: 0.6797 - accuracy: 0.566 - ETA: 49s - loss: 0.6803 - accuracy: 0.565 - ETA: 48s - loss: 0.6806 - accuracy: 0.564 - ETA: 46s - loss: 0.6809 - accuracy: 0.564 - ETA: 45s - loss: 0.6810 - accuracy: 0.564 - ETA: 43s - loss: 0.6811 - accuracy: 0.562 - ETA: 42s - loss: 0.6812 - accuracy: 0.562 - ETA: 41s - loss: 0.6814 - accuracy: 0.562 - ETA: 39s - loss: 0.6817 - accuracy: 0.561 - ETA: 38s - loss: 0.6813 - accuracy: 0.562 - ETA: 36s - loss: 0.6813 - accuracy: 0.563 - ETA: 35s - loss: 0.6810 - accuracy: 0.563 - ETA: 33s - loss: 0.6812 - accuracy: 0.562 - ETA: 32s - loss: 0.6812 - accuracy: 0.562 - ETA: 30s - loss: 0.6814 - accuracy: 0.562 - ETA: 29s - loss: 0.6814 - accuracy: 0.562 - ETA: 27s - loss: 0.6816 - accuracy: 0.562 - ETA: 26s - loss: 0.6811 - accuracy: 0.563 - ETA: 24s - loss: 0.6802 - accuracy: 0.565 - ETA: 23s - loss: 0.6796 - accuracy: 0.566 - ETA: 22s - loss: 0.6797 - accuracy: 0.566 - ETA: 20s - loss: 0.6798 - accuracy: 0.565 - ETA: 19s - loss: 0.6799 - accuracy: 0.566 - ETA: 17s - loss: 0.6798 - accuracy: 0.566 - ETA: 16s - loss: 0.6792 - accuracy: 0.567 - ETA: 14s - loss: 0.6794 - accuracy: 0.567 - ETA: 13s - loss: 0.6786 - accuracy: 0.570 - ETA: 11s - loss: 0.6784 - accuracy: 0.570 - ETA: 10s - loss: 0.6792 - accuracy: 0.567 - ETA: 8s - loss: 0.6789 - accuracy: 0.568 - ETA: 7s - loss: 0.6788 - accuracy: 0.56 - ETA: 5s - loss: 0.6785 - accuracy: 0.56 - ETA: 4s - loss: 0.6782 - accuracy: 0.56 - ETA: 2s - loss: 0.6780 - accuracy: 0.57 - ETA: 1s - loss: 0.6773 - accuracy: 0.57 - ETA: 0s - loss: 0.6768 - accuracy: 0.57 - 190s 2s/step - loss: 0.6768 - accuracy: 0.5727 - val_loss: 0.6572 - val_accuracy: 0.6078\n",
      "Epoch 4/4\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.6695 - accuracy: 0.59 - ETA: 1:23 - loss: 0.6862 - accuracy: 0.56 - ETA: 1:50 - loss: 0.7002 - accuracy: 0.56 - ETA: 2:03 - loss: 0.6795 - accuracy: 0.60 - ETA: 2:10 - loss: 0.6889 - accuracy: 0.58 - ETA: 2:14 - loss: 0.6864 - accuracy: 0.59 - ETA: 2:17 - loss: 0.6789 - accuracy: 0.60 - ETA: 2:18 - loss: 0.6798 - accuracy: 0.60 - ETA: 2:19 - loss: 0.6800 - accuracy: 0.61 - ETA: 2:20 - loss: 0.6811 - accuracy: 0.60 - ETA: 2:20 - loss: 0.6806 - accuracy: 0.60 - ETA: 2:20 - loss: 0.6808 - accuracy: 0.60 - ETA: 2:19 - loss: 0.6820 - accuracy: 0.60 - ETA: 2:19 - loss: 0.6809 - accuracy: 0.60 - ETA: 2:18 - loss: 0.6773 - accuracy: 0.60 - ETA: 2:17 - loss: 0.6731 - accuracy: 0.61 - ETA: 2:16 - loss: 0.6702 - accuracy: 0.61 - ETA: 2:15 - loss: 0.6707 - accuracy: 0.60 - ETA: 2:14 - loss: 0.6713 - accuracy: 0.60 - ETA: 2:13 - loss: 0.6712 - accuracy: 0.60 - ETA: 2:12 - loss: 0.6707 - accuracy: 0.60 - ETA: 2:11 - loss: 0.6701 - accuracy: 0.60 - ETA: 2:10 - loss: 0.6697 - accuracy: 0.60 - ETA: 2:09 - loss: 0.6697 - accuracy: 0.60 - ETA: 2:08 - loss: 0.6709 - accuracy: 0.59 - ETA: 2:07 - loss: 0.6698 - accuracy: 0.59 - ETA: 2:06 - loss: 0.6707 - accuracy: 0.59 - ETA: 2:05 - loss: 0.6693 - accuracy: 0.60 - ETA: 2:03 - loss: 0.6681 - accuracy: 0.60 - ETA: 2:02 - loss: 0.6681 - accuracy: 0.60 - ETA: 2:01 - loss: 0.6670 - accuracy: 0.60 - ETA: 1:59 - loss: 0.6659 - accuracy: 0.60 - ETA: 1:58 - loss: 0.6645 - accuracy: 0.60 - ETA: 1:57 - loss: 0.6658 - accuracy: 0.60 - ETA: 1:55 - loss: 0.6670 - accuracy: 0.60 - ETA: 1:54 - loss: 0.6683 - accuracy: 0.60 - ETA: 1:53 - loss: 0.6684 - accuracy: 0.60 - ETA: 1:51 - loss: 0.6690 - accuracy: 0.60 - ETA: 1:50 - loss: 0.6693 - accuracy: 0.60 - ETA: 1:48 - loss: 0.6690 - accuracy: 0.60 - ETA: 1:47 - loss: 0.6710 - accuracy: 0.59 - ETA: 1:46 - loss: 0.6704 - accuracy: 0.59 - ETA: 1:44 - loss: 0.6700 - accuracy: 0.59 - ETA: 1:43 - loss: 0.6702 - accuracy: 0.60 - ETA: 1:42 - loss: 0.6710 - accuracy: 0.59 - ETA: 1:40 - loss: 0.6714 - accuracy: 0.59 - ETA: 1:39 - loss: 0.6719 - accuracy: 0.59 - ETA: 1:37 - loss: 0.6715 - accuracy: 0.59 - ETA: 1:36 - loss: 0.6705 - accuracy: 0.59 - ETA: 1:34 - loss: 0.6708 - accuracy: 0.59 - ETA: 1:33 - loss: 0.6702 - accuracy: 0.59 - ETA: 1:32 - loss: 0.6702 - accuracy: 0.59 - ETA: 1:30 - loss: 0.6695 - accuracy: 0.59 - ETA: 1:29 - loss: 0.6698 - accuracy: 0.59 - ETA: 1:27 - loss: 0.6706 - accuracy: 0.59 - ETA: 1:26 - loss: 0.6710 - accuracy: 0.59 - ETA: 1:24 - loss: 0.6707 - accuracy: 0.59 - ETA: 1:23 - loss: 0.6711 - accuracy: 0.59 - ETA: 1:22 - loss: 0.6700 - accuracy: 0.59 - ETA: 1:20 - loss: 0.6700 - accuracy: 0.59 - ETA: 1:19 - loss: 0.6703 - accuracy: 0.59 - ETA: 1:17 - loss: 0.6707 - accuracy: 0.59 - ETA: 1:16 - loss: 0.6699 - accuracy: 0.59 - ETA: 1:14 - loss: 0.6699 - accuracy: 0.59 - ETA: 1:13 - loss: 0.6698 - accuracy: 0.59 - ETA: 1:11 - loss: 0.6696 - accuracy: 0.59 - ETA: 1:10 - loss: 0.6690 - accuracy: 0.59 - ETA: 1:08 - loss: 0.6689 - accuracy: 0.59 - ETA: 1:07 - loss: 0.6685 - accuracy: 0.59 - ETA: 1:06 - loss: 0.6678 - accuracy: 0.59 - ETA: 1:04 - loss: 0.6684 - accuracy: 0.59 - ETA: 1:03 - loss: 0.6683 - accuracy: 0.59 - ETA: 1:01 - loss: 0.6676 - accuracy: 0.59 - ETA: 1:00 - loss: 0.6674 - accuracy: 0.59 - ETA: 58s - loss: 0.6673 - accuracy: 0.5975 - ETA: 57s - loss: 0.6679 - accuracy: 0.597 - ETA: 55s - loss: 0.6681 - accuracy: 0.597 - ETA: 54s - loss: 0.6686 - accuracy: 0.596 - ETA: 52s - loss: 0.6678 - accuracy: 0.598 - ETA: 51s - loss: 0.6672 - accuracy: 0.600 - ETA: 50s - loss: 0.6667 - accuracy: 0.601 - ETA: 48s - loss: 0.6662 - accuracy: 0.602 - ETA: 47s - loss: 0.6659 - accuracy: 0.601 - ETA: 45s - loss: 0.6663 - accuracy: 0.600 - ETA: 44s - loss: 0.6672 - accuracy: 0.598 - ETA: 42s - loss: 0.6662 - accuracy: 0.599 - ETA: 41s - loss: 0.6664 - accuracy: 0.599 - ETA: 39s - loss: 0.6663 - accuracy: 0.599 - ETA: 38s - loss: 0.6660 - accuracy: 0.599 - ETA: 36s - loss: 0.6662 - accuracy: 0.600 - ETA: 35s - loss: 0.6659 - accuracy: 0.600 - ETA: 33s - loss: 0.6658 - accuracy: 0.600 - ETA: 32s - loss: 0.6656 - accuracy: 0.601 - ETA: 30s - loss: 0.6645 - accuracy: 0.602 - ETA: 29s - loss: 0.6640 - accuracy: 0.604 - ETA: 28s - loss: 0.6640 - accuracy: 0.604 - ETA: 26s - loss: 0.6644 - accuracy: 0.604 - ETA: 25s - loss: 0.6644 - accuracy: 0.603 - ETA: 23s - loss: 0.6644 - accuracy: 0.604 - ETA: 22s - loss: 0.6640 - accuracy: 0.604 - ETA: 20s - loss: 0.6640 - accuracy: 0.604 - ETA: 19s - loss: 0.6631 - accuracy: 0.606 - ETA: 17s - loss: 0.6625 - accuracy: 0.607 - ETA: 16s - loss: 0.6624 - accuracy: 0.607 - ETA: 14s - loss: 0.6624 - accuracy: 0.607 - ETA: 13s - loss: 0.6627 - accuracy: 0.606 - ETA: 11s - loss: 0.6630 - accuracy: 0.605 - ETA: 10s - loss: 0.6637 - accuracy: 0.604 - ETA: 8s - loss: 0.6634 - accuracy: 0.603 - ETA: 7s - loss: 0.6626 - accuracy: 0.60 - ETA: 5s - loss: 0.6627 - accuracy: 0.60 - ETA: 4s - loss: 0.6630 - accuracy: 0.60 - ETA: 2s - loss: 0.6623 - accuracy: 0.60 - ETA: 1s - loss: 0.6628 - accuracy: 0.60 - ETA: 0s - loss: 0.6630 - accuracy: 0.60 - 187s 2s/step - loss: 0.6630 - accuracy: 0.6037 - val_loss: 0.6491 - val_accuracy: 0.6176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 780d2c81f53edd4b0de595bc89305005</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.6176470518112183</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dense_units: 256</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dropout_1: 0.2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dropout_2: 0.4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-learning_rate: 3e-05</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-lstm2_units: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-optimizer_name: rmsprop</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/bracket: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/epochs: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/initial_epoch: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/round: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/trial_id: 17fc52a28638747129eb4629a2cc7c42</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_idx (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segments (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 512, 768), ( 109482240   input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 input_segments[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 109,679,361\n",
      "Trainable params: 197,121\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 3/4\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.6369 - accuracy: 0.71 - ETA: 1:22 - loss: 0.6556 - accuracy: 0.64 - ETA: 1:48 - loss: 0.6743 - accuracy: 0.61 - ETA: 2:01 - loss: 0.6755 - accuracy: 0.62 - ETA: 2:08 - loss: 0.6787 - accuracy: 0.61 - ETA: 2:12 - loss: 0.6824 - accuracy: 0.59 - ETA: 2:15 - loss: 0.6869 - accuracy: 0.58 - ETA: 2:17 - loss: 0.6901 - accuracy: 0.57 - ETA: 2:18 - loss: 0.6900 - accuracy: 0.57 - ETA: 2:18 - loss: 0.6946 - accuracy: 0.57 - ETA: 2:18 - loss: 0.6956 - accuracy: 0.57 - ETA: 2:18 - loss: 0.6999 - accuracy: 0.56 - ETA: 2:18 - loss: 0.6975 - accuracy: 0.56 - ETA: 2:17 - loss: 0.6956 - accuracy: 0.56 - ETA: 2:17 - loss: 0.6941 - accuracy: 0.57 - ETA: 2:16 - loss: 0.6945 - accuracy: 0.56 - ETA: 2:15 - loss: 0.6952 - accuracy: 0.55 - ETA: 2:14 - loss: 0.6931 - accuracy: 0.56 - ETA: 2:13 - loss: 0.6913 - accuracy: 0.56 - ETA: 2:12 - loss: 0.6912 - accuracy: 0.57 - ETA: 2:11 - loss: 0.6875 - accuracy: 0.57 - ETA: 2:10 - loss: 0.6857 - accuracy: 0.58 - ETA: 2:09 - loss: 0.6878 - accuracy: 0.58 - ETA: 2:08 - loss: 0.6882 - accuracy: 0.57 - ETA: 2:07 - loss: 0.6868 - accuracy: 0.58 - ETA: 2:06 - loss: 0.6866 - accuracy: 0.58 - ETA: 2:05 - loss: 0.6876 - accuracy: 0.57 - ETA: 2:03 - loss: 0.6880 - accuracy: 0.57 - ETA: 2:02 - loss: 0.6868 - accuracy: 0.57 - ETA: 2:01 - loss: 0.6851 - accuracy: 0.58 - ETA: 2:00 - loss: 0.6858 - accuracy: 0.57 - ETA: 1:58 - loss: 0.6854 - accuracy: 0.58 - ETA: 1:57 - loss: 0.6857 - accuracy: 0.57 - ETA: 1:56 - loss: 0.6859 - accuracy: 0.57 - ETA: 1:54 - loss: 0.6871 - accuracy: 0.57 - ETA: 1:53 - loss: 0.6862 - accuracy: 0.57 - ETA: 1:52 - loss: 0.6867 - accuracy: 0.57 - ETA: 1:50 - loss: 0.6860 - accuracy: 0.57 - ETA: 1:49 - loss: 0.6868 - accuracy: 0.57 - ETA: 1:48 - loss: 0.6863 - accuracy: 0.57 - ETA: 1:46 - loss: 0.6858 - accuracy: 0.57 - ETA: 1:45 - loss: 0.6856 - accuracy: 0.57 - ETA: 1:44 - loss: 0.6862 - accuracy: 0.57 - ETA: 1:42 - loss: 0.6884 - accuracy: 0.57 - ETA: 1:41 - loss: 0.6887 - accuracy: 0.56 - ETA: 1:39 - loss: 0.6900 - accuracy: 0.56 - ETA: 1:38 - loss: 0.6888 - accuracy: 0.56 - ETA: 1:37 - loss: 0.6894 - accuracy: 0.56 - ETA: 1:35 - loss: 0.6893 - accuracy: 0.56 - ETA: 1:34 - loss: 0.6890 - accuracy: 0.56 - ETA: 1:33 - loss: 0.6898 - accuracy: 0.56 - ETA: 1:31 - loss: 0.6897 - accuracy: 0.56 - ETA: 1:30 - loss: 0.6905 - accuracy: 0.56 - ETA: 1:28 - loss: 0.6916 - accuracy: 0.56 - ETA: 1:27 - loss: 0.6917 - accuracy: 0.56 - ETA: 1:25 - loss: 0.6919 - accuracy: 0.56 - ETA: 1:24 - loss: 0.6911 - accuracy: 0.56 - ETA: 1:23 - loss: 0.6902 - accuracy: 0.56 - ETA: 1:21 - loss: 0.6899 - accuracy: 0.56 - ETA: 1:20 - loss: 0.6902 - accuracy: 0.56 - ETA: 1:18 - loss: 0.6900 - accuracy: 0.56 - ETA: 1:17 - loss: 0.6901 - accuracy: 0.56 - ETA: 1:15 - loss: 0.6916 - accuracy: 0.56 - ETA: 1:14 - loss: 0.6914 - accuracy: 0.56 - ETA: 1:13 - loss: 0.6914 - accuracy: 0.56 - ETA: 1:11 - loss: 0.6902 - accuracy: 0.56 - ETA: 1:10 - loss: 0.6906 - accuracy: 0.56 - ETA: 1:08 - loss: 0.6909 - accuracy: 0.56 - ETA: 1:07 - loss: 0.6913 - accuracy: 0.56 - ETA: 1:05 - loss: 0.6911 - accuracy: 0.56 - ETA: 1:04 - loss: 0.6897 - accuracy: 0.56 - ETA: 1:02 - loss: 0.6889 - accuracy: 0.56 - ETA: 1:01 - loss: 0.6884 - accuracy: 0.56 - ETA: 1:00 - loss: 0.6889 - accuracy: 0.56 - ETA: 58s - loss: 0.6883 - accuracy: 0.5679 - ETA: 57s - loss: 0.6886 - accuracy: 0.567 - ETA: 55s - loss: 0.6890 - accuracy: 0.566 - ETA: 54s - loss: 0.6885 - accuracy: 0.566 - ETA: 52s - loss: 0.6887 - accuracy: 0.567 - ETA: 51s - loss: 0.6889 - accuracy: 0.566 - ETA: 49s - loss: 0.6890 - accuracy: 0.566 - ETA: 48s - loss: 0.6896 - accuracy: 0.565 - ETA: 46s - loss: 0.6897 - accuracy: 0.564 - ETA: 45s - loss: 0.6894 - accuracy: 0.565 - ETA: 44s - loss: 0.6896 - accuracy: 0.565 - ETA: 42s - loss: 0.6902 - accuracy: 0.564 - ETA: 41s - loss: 0.6896 - accuracy: 0.564 - ETA: 39s - loss: 0.6894 - accuracy: 0.565 - ETA: 38s - loss: 0.6893 - accuracy: 0.564 - ETA: 36s - loss: 0.6890 - accuracy: 0.566 - ETA: 35s - loss: 0.6888 - accuracy: 0.565 - ETA: 33s - loss: 0.6886 - accuracy: 0.565 - ETA: 32s - loss: 0.6886 - accuracy: 0.565 - ETA: 30s - loss: 0.6891 - accuracy: 0.563 - ETA: 29s - loss: 0.6889 - accuracy: 0.564 - ETA: 27s - loss: 0.6889 - accuracy: 0.564 - ETA: 26s - loss: 0.6888 - accuracy: 0.564 - ETA: 25s - loss: 0.6889 - accuracy: 0.564 - ETA: 23s - loss: 0.6887 - accuracy: 0.563 - ETA: 22s - loss: 0.6892 - accuracy: 0.562 - ETA: 20s - loss: 0.6894 - accuracy: 0.562 - ETA: 19s - loss: 0.6894 - accuracy: 0.563 - ETA: 17s - loss: 0.6892 - accuracy: 0.563 - ETA: 16s - loss: 0.6889 - accuracy: 0.563 - ETA: 14s - loss: 0.6891 - accuracy: 0.562 - ETA: 13s - loss: 0.6890 - accuracy: 0.562 - ETA: 11s - loss: 0.6892 - accuracy: 0.561 - ETA: 10s - loss: 0.6895 - accuracy: 0.561 - ETA: 8s - loss: 0.6895 - accuracy: 0.561 - ETA: 7s - loss: 0.6894 - accuracy: 0.56 - ETA: 5s - loss: 0.6891 - accuracy: 0.56 - ETA: 4s - loss: 0.6888 - accuracy: 0.56 - ETA: 2s - loss: 0.6885 - accuracy: 0.56 - ETA: 1s - loss: 0.6880 - accuracy: 0.56 - ETA: 0s - loss: 0.6877 - accuracy: 0.56 - 190s 2s/step - loss: 0.6877 - accuracy: 0.5631 - val_loss: 0.6680 - val_accuracy: 0.5931\n",
      "Epoch 4/4\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.6590 - accuracy: 0.50 - ETA: 1:25 - loss: 0.6785 - accuracy: 0.50 - ETA: 1:53 - loss: 0.6943 - accuracy: 0.51 - ETA: 2:06 - loss: 0.6767 - accuracy: 0.56 - ETA: 2:12 - loss: 0.6790 - accuracy: 0.56 - ETA: 2:16 - loss: 0.6733 - accuracy: 0.57 - ETA: 2:18 - loss: 0.6694 - accuracy: 0.58 - ETA: 2:20 - loss: 0.6715 - accuracy: 0.57 - ETA: 2:20 - loss: 0.6763 - accuracy: 0.55 - ETA: 2:21 - loss: 0.6790 - accuracy: 0.55 - ETA: 2:21 - loss: 0.6799 - accuracy: 0.55 - ETA: 2:20 - loss: 0.6795 - accuracy: 0.55 - ETA: 2:20 - loss: 0.6816 - accuracy: 0.54 - ETA: 2:20 - loss: 0.6827 - accuracy: 0.54 - ETA: 2:19 - loss: 0.6845 - accuracy: 0.54 - ETA: 2:18 - loss: 0.6826 - accuracy: 0.55 - ETA: 2:18 - loss: 0.6803 - accuracy: 0.56 - ETA: 2:17 - loss: 0.6798 - accuracy: 0.57 - ETA: 2:15 - loss: 0.6830 - accuracy: 0.56 - ETA: 2:14 - loss: 0.6841 - accuracy: 0.56 - ETA: 2:13 - loss: 0.6842 - accuracy: 0.56 - ETA: 2:12 - loss: 0.6820 - accuracy: 0.56 - ETA: 2:11 - loss: 0.6810 - accuracy: 0.56 - ETA: 2:10 - loss: 0.6794 - accuracy: 0.57 - ETA: 2:08 - loss: 0.6773 - accuracy: 0.57 - ETA: 2:07 - loss: 0.6778 - accuracy: 0.57 - ETA: 2:06 - loss: 0.6774 - accuracy: 0.57 - ETA: 2:05 - loss: 0.6770 - accuracy: 0.57 - ETA: 2:03 - loss: 0.6735 - accuracy: 0.57 - ETA: 2:02 - loss: 0.6740 - accuracy: 0.57 - ETA: 2:01 - loss: 0.6731 - accuracy: 0.58 - ETA: 1:59 - loss: 0.6721 - accuracy: 0.58 - ETA: 1:58 - loss: 0.6706 - accuracy: 0.58 - ETA: 1:57 - loss: 0.6717 - accuracy: 0.58 - ETA: 1:55 - loss: 0.6730 - accuracy: 0.58 - ETA: 1:54 - loss: 0.6740 - accuracy: 0.58 - ETA: 1:52 - loss: 0.6745 - accuracy: 0.58 - ETA: 1:51 - loss: 0.6758 - accuracy: 0.57 - ETA: 1:50 - loss: 0.6762 - accuracy: 0.57 - ETA: 1:48 - loss: 0.6770 - accuracy: 0.57 - ETA: 1:47 - loss: 0.6773 - accuracy: 0.57 - ETA: 1:46 - loss: 0.6763 - accuracy: 0.57 - ETA: 1:44 - loss: 0.6760 - accuracy: 0.57 - ETA: 1:43 - loss: 0.6759 - accuracy: 0.57 - ETA: 1:41 - loss: 0.6764 - accuracy: 0.57 - ETA: 1:40 - loss: 0.6762 - accuracy: 0.57 - ETA: 1:39 - loss: 0.6759 - accuracy: 0.57 - ETA: 1:37 - loss: 0.6762 - accuracy: 0.57 - ETA: 1:36 - loss: 0.6764 - accuracy: 0.57 - ETA: 1:34 - loss: 0.6764 - accuracy: 0.57 - ETA: 1:33 - loss: 0.6760 - accuracy: 0.57 - ETA: 1:31 - loss: 0.6755 - accuracy: 0.57 - ETA: 1:30 - loss: 0.6751 - accuracy: 0.57 - ETA: 1:29 - loss: 0.6753 - accuracy: 0.57 - ETA: 1:27 - loss: 0.6755 - accuracy: 0.57 - ETA: 1:26 - loss: 0.6760 - accuracy: 0.57 - ETA: 1:24 - loss: 0.6755 - accuracy: 0.57 - ETA: 1:23 - loss: 0.6750 - accuracy: 0.57 - ETA: 1:22 - loss: 0.6749 - accuracy: 0.57 - ETA: 1:20 - loss: 0.6747 - accuracy: 0.57 - ETA: 1:19 - loss: 0.6751 - accuracy: 0.57 - ETA: 1:17 - loss: 0.6753 - accuracy: 0.57 - ETA: 1:16 - loss: 0.6754 - accuracy: 0.57 - ETA: 1:14 - loss: 0.6753 - accuracy: 0.57 - ETA: 1:13 - loss: 0.6747 - accuracy: 0.57 - ETA: 1:11 - loss: 0.6749 - accuracy: 0.57 - ETA: 1:10 - loss: 0.6749 - accuracy: 0.57 - ETA: 1:08 - loss: 0.6742 - accuracy: 0.57 - ETA: 1:07 - loss: 0.6742 - accuracy: 0.57 - ETA: 1:06 - loss: 0.6738 - accuracy: 0.57 - ETA: 1:04 - loss: 0.6741 - accuracy: 0.57 - ETA: 1:03 - loss: 0.6736 - accuracy: 0.57 - ETA: 1:01 - loss: 0.6736 - accuracy: 0.57 - ETA: 1:00 - loss: 0.6742 - accuracy: 0.57 - ETA: 58s - loss: 0.6738 - accuracy: 0.5792 - ETA: 57s - loss: 0.6742 - accuracy: 0.578 - ETA: 55s - loss: 0.6744 - accuracy: 0.579 - ETA: 54s - loss: 0.6746 - accuracy: 0.578 - ETA: 52s - loss: 0.6739 - accuracy: 0.580 - ETA: 51s - loss: 0.6739 - accuracy: 0.580 - ETA: 49s - loss: 0.6738 - accuracy: 0.581 - ETA: 48s - loss: 0.6732 - accuracy: 0.581 - ETA: 47s - loss: 0.6729 - accuracy: 0.582 - ETA: 45s - loss: 0.6733 - accuracy: 0.580 - ETA: 44s - loss: 0.6737 - accuracy: 0.580 - ETA: 42s - loss: 0.6733 - accuracy: 0.580 - ETA: 41s - loss: 0.6734 - accuracy: 0.580 - ETA: 39s - loss: 0.6736 - accuracy: 0.579 - ETA: 38s - loss: 0.6734 - accuracy: 0.579 - ETA: 36s - loss: 0.6731 - accuracy: 0.580 - ETA: 35s - loss: 0.6729 - accuracy: 0.581 - ETA: 33s - loss: 0.6733 - accuracy: 0.580 - ETA: 32s - loss: 0.6729 - accuracy: 0.580 - ETA: 30s - loss: 0.6726 - accuracy: 0.581 - ETA: 29s - loss: 0.6718 - accuracy: 0.583 - ETA: 27s - loss: 0.6717 - accuracy: 0.584 - ETA: 26s - loss: 0.6718 - accuracy: 0.584 - ETA: 25s - loss: 0.6719 - accuracy: 0.584 - ETA: 23s - loss: 0.6723 - accuracy: 0.584 - ETA: 22s - loss: 0.6722 - accuracy: 0.585 - ETA: 20s - loss: 0.6724 - accuracy: 0.584 - ETA: 19s - loss: 0.6717 - accuracy: 0.585 - ETA: 17s - loss: 0.6712 - accuracy: 0.586 - ETA: 16s - loss: 0.6717 - accuracy: 0.585 - ETA: 14s - loss: 0.6717 - accuracy: 0.585 - ETA: 13s - loss: 0.6720 - accuracy: 0.584 - ETA: 11s - loss: 0.6719 - accuracy: 0.585 - ETA: 10s - loss: 0.6723 - accuracy: 0.584 - ETA: 8s - loss: 0.6722 - accuracy: 0.584 - ETA: 7s - loss: 0.6717 - accuracy: 0.58 - ETA: 5s - loss: 0.6716 - accuracy: 0.58 - ETA: 4s - loss: 0.6718 - accuracy: 0.58 - ETA: 2s - loss: 0.6715 - accuracy: 0.58 - ETA: 1s - loss: 0.6719 - accuracy: 0.58 - ETA: 0s - loss: 0.6718 - accuracy: 0.58 - 187s 2s/step - loss: 0.6718 - accuracy: 0.5838 - val_loss: 0.6554 - val_accuracy: 0.6103\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 3d31b0bf6dac4e8447a3f460d13d767f</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.6102941036224365</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dense_units: 256</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dropout_1: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dropout_2: 0.30000000000000004</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-learning_rate: 2e-05</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-lstm2_units: 128</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-optimizer_name: adam</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/bracket: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/epochs: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/initial_epoch: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/round: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/trial_id: 3f58a852205a3b52cc613ff41f138332</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_idx (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segments (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 512, 768), ( 109482240   input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 input_segments[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 109,679,361\n",
      "Trainable params: 197,121\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[32,12,512,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/tf_bert_model/bert/encoder/layer_._0/attention/self/Softmax (defined at /home/malves/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/transformers/modeling_tf_bert.py:291) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_766605]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/tf_bert_model/bert/encoder/layer_._0/attention/self/Softmax:\n model/tf_bert_model/bert/encoder/layer_._0/attention/self/add (defined at /home/malves/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/transformers/modeling_tf_bert.py:288)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a9917e348601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     overwrite=True)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/kerastuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/kerastuner/tuners/hyperband.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tuner/epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'initial_epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tuner/initial_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHyperband\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/kerastuner/engine/multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'min'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[32,12,512,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/tf_bert_model/bert/encoder/layer_._0/attention/self/Softmax (defined at /home/malves/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/transformers/modeling_tf_bert.py:291) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_766605]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model/tf_bert_model/bert/encoder/layer_._0/attention/self/Softmax:\n model/tf_bert_model/bert/encoder/layer_._0/attention/self/add (defined at /home/malves/miniconda3/envs/biotmpygpu/lib/python3.8/site-packages/transformers/modeling_tf_bert.py:288)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "tuner = Hyperband(\n",
    "    Bert_hyper,\n",
    "    max_epochs=4,\n",
    "    objective='val_accuracy',\n",
    "    seed = seed_value,\n",
    "    directory=model_name,\n",
    "    overwrite=True)\n",
    "\n",
    "tuner.search(x_train,y_train, epochs=4, batch_size = 32, validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lstm2_units': 64,\n",
       " 'dropout_1': 0.2,\n",
       " 'dense_units': 256,\n",
       " 'dropout_2': 0.4,\n",
       " 'optimizer_name': 'rmsprop',\n",
       " 'learning_rate': 3e-05,\n",
       " 'tuner/epochs': 2,\n",
       " 'tuner/initial_epoch': 0,\n",
       " 'tuner/bracket': 1,\n",
       " 'tuner/round': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_idx (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segments (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 512, 768), ( 109482240   input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 input_segments[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 109,679,361\n",
      "Trainable params: 197,121\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_idx (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segments (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 512, 768), ( 109482240   input_idx[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 input_segments[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 109,679,361\n",
      "Trainable params: 197,121\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
